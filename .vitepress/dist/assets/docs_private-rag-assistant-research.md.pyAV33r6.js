import{_ as e,o as s,c as n,a0 as r}from"./chunks/framework.B3sz4m_N.js";const u=JSON.parse('{"title":"Private RAG Assistant for WordPress + Laravel Dev Team","description":"","frontmatter":{},"headers":[],"relativePath":"docs/private-rag-assistant-research.md","filePath":"docs/private-rag-assistant-research.md"}'),a={name:"docs/private-rag-assistant-research.md"};function o(d,t,i,l,c,p){return s(),n("div",null,[...t[0]||(t[0]=[r(`<h1 id="private-rag-assistant-for-wordpress-laravel-dev-team" tabindex="-1">Private RAG Assistant for WordPress + Laravel Dev Team <a class="header-anchor" href="#private-rag-assistant-for-wordpress-laravel-dev-team" aria-label="Permalink to &quot;Private RAG Assistant for WordPress + Laravel Dev Team&quot;">​</a></h1><p><strong>Research document</strong> — Deployment runbooks, Jira exports, infrastructure docs, plugin documentation, internal Markdown.</p><hr><h2 id="_1-embedding-model-selection" tabindex="-1">1. Embedding model selection <a class="header-anchor" href="#_1-embedding-model-selection" aria-label="Permalink to &quot;1. Embedding model selection&quot;">​</a></h2><table tabindex="0"><thead><tr><th>Criterion</th><th>Recommendation</th><th>Notes</th></tr></thead><tbody><tr><td><strong>Best balance (POC)</strong></td><td><strong>e5-base-v2</strong> or <strong>BAAI/bge-base-en-v1.5</strong></td><td>~100M params, 100% Top-5 accuracy in benchmarks, low latency (~16–50 ms).</td></tr><tr><td><strong>Highest retrieval quality</strong></td><td><strong>e5-large-instruct</strong> or <strong>BGE-large</strong></td><td>85%+ retrieval accuracy; stronger for mixed code + prose.</td></tr><tr><td><strong>Lightweight / CPU-only</strong></td><td><strong>e5-small</strong> (118M)</td><td>Fastest (16 ms), good Top-5 accuracy; suitable for small teams or dev machines.</td></tr><tr><td><strong>Self-hosted, popular</strong></td><td><strong>nomic-embed-text</strong> (768 dims)</td><td>Well-supported, good for docs + code; fits Ollama/local stacks.</td></tr></tbody></table><p><strong>Avoid for production:</strong> <code>all-MiniLM-L6-v2</code> (outdated, ~56% Top-5 accuracy).</p><p><strong>Self-hosting:</strong> Use <strong>Ollama</strong> for local inference or <strong>Sentence Transformers</strong> in Python; keep embeddings on your infra to satisfy data residency.</p><p><strong>For WordPress/Laravel:</strong> Prefer <strong>multilingual-capable</strong> models (e.g. BGE, e5) if runbooks or Jira use mixed languages; otherwise English-focused models (e5-*, BGE-en) are sufficient.</p><hr><h2 id="_2-vector-database-comparison" tabindex="-1">2. Vector database comparison <a class="header-anchor" href="#_2-vector-database-comparison" aria-label="Permalink to &quot;2. Vector database comparison&quot;">​</a></h2><table tabindex="0"><thead><tr><th>Database</th><th>Self-hosted</th><th>Latency (P95)</th><th>Indexing (1M vectors)</th><th>Typical cost (self-hosted)</th><th>Best for</th></tr></thead><tbody><tr><td><strong>Qdrant</strong></td><td>✅</td><td>22–38 ms</td><td>~6 min</td><td>~$120/mo (r6g.xlarge)</td><td>Performance, filtering, Rust/APACHE 2.0</td></tr><tr><td><strong>Weaviate</strong></td><td>✅</td><td>38–65 ms</td><td>~9 min</td><td>~$120/mo (r6g.xlarge)</td><td>Hybrid (vector + keyword), GraphQL, multi-tenant</td></tr><tr><td><strong>Chroma</strong></td><td>✅</td><td>180–340 ms</td><td>~28 min</td><td>~$60/mo (t3.large)</td><td>Prototyping, local dev, tight budget</td></tr><tr><td><strong>Pinecone</strong></td><td>❌ managed only</td><td>—</td><td>—</td><td>~$3k/mo (performance tier)</td><td>Skip for “private” requirement</td></tr></tbody></table><p><strong>Recommendation for POC:</strong> <strong>Qdrant</strong> (Docker, low ops) or <strong>Chroma</strong> (fastest to stand up). For production with hybrid search (Jira + code + Markdown), <strong>Weaviate</strong> is a strong option.</p><p><strong>Recall:</strong> All in the 0.94–0.98 range for typical RAG; choice matters more for scale, filtering, and ops.</p><hr><h2 id="_3-chunking-strategies-code-vs-docs" tabindex="-1">3. Chunking strategies: code vs docs <a class="header-anchor" href="#_3-chunking-strategies-code-vs-docs" aria-label="Permalink to &quot;3. Chunking strategies: code vs docs&quot;">​</a></h2><h3 id="documentation-runbooks-infra-docs-markdown-jira-exports" tabindex="-1">Documentation (runbooks, infra docs, Markdown, Jira exports) <a class="header-anchor" href="#documentation-runbooks-infra-docs-markdown-jira-exports" aria-label="Permalink to &quot;Documentation (runbooks, infra docs, Markdown, Jira exports)&quot;">​</a></h3><ul><li><strong>Preferred:</strong> <strong>Semantic chunking</strong> — split at meaning boundaries (sections, paragraphs) rather than fixed token counts. <ul><li><strong>Embedding-based:</strong> Compute similarity between adjacent sentences; split when similarity drops (e.g. cosine &lt; threshold). ~85–91% retrieval accuracy vs ~75–80% for fixed-size.</li><li><strong>NLP-based:</strong> Use spaCy (or similar) for section/topic boundaries.</li></ul></li><li><strong>Overlap:</strong> 50–100 tokens overlap at chunk boundaries to avoid cutting context.</li><li><strong>Sizes:</strong> Variable; aim for ~200–800 tokens per chunk so full procedures stay together.</li></ul><h3 id="source-code-plugins-laravel-app-code" tabindex="-1">Source code (plugins, Laravel app code) <a class="header-anchor" href="#source-code-plugins-laravel-app-code" aria-label="Permalink to &quot;Source code (plugins, Laravel app code)&quot;">​</a></h3><ul><li><strong>Preferred:</strong> <strong>AST-based chunking</strong> (e.g. <strong>cAST</strong>, <strong>Tree-sitter</strong>). <ul><li>Preserve <strong>function/class/module</strong> boundaries; avoid splitting mid-function.</li><li>Tree-sitter supports 29+ languages (PHP, JavaScript, etc.) and fits WordPress/Laravel stacks.</li><li><strong>Results:</strong> ~4.3 pt gain Recall@5 vs fixed-size; ~2.67 pt Pass@1 on code-generation tasks.</li></ul></li><li><strong>Fallback:</strong> Recursive character/text splitting with <strong>code-aware separators</strong> (e.g. <code>\\n\\n</code>, <code>function </code>, <code>class </code>) and max chunk size (~512 tokens).</li></ul><h3 id="mixed-content-plugin-docs-code-snippets" tabindex="-1">Mixed content (plugin docs + code snippets) <a class="header-anchor" href="#mixed-content-plugin-docs-code-snippets" aria-label="Permalink to &quot;Mixed content (plugin docs + code snippets)&quot;">​</a></h3><ul><li>Use <strong>two pipelines:</strong> one semantic for prose, one AST for code; tag chunks with <code>type: doc | code</code> and <strong>language</strong> for filtering.</li><li>Store <strong>metadata:</strong> <code>source</code> (e.g. repo, Confluence, Jira), <code>doc_type</code> (runbook, API, README), <code>project</code> (WordPress plugin X, Laravel app Y).</li></ul><hr><h2 id="_4-security-and-access-control" tabindex="-1">4. Security and access control <a class="header-anchor" href="#_4-security-and-access-control" aria-label="Permalink to &quot;4. Security and access control&quot;">​</a></h2><ul><li><strong>Authentication:</strong> Central IdP (e.g. <strong>SSO/SAML/OIDC</strong>). No shared API keys for end-users.</li><li><strong>Authorization:</strong> Apply <strong>RBAC</strong> (or ABAC/ReBAC if needed) so retrieval only sees documents the user is allowed to see. <ul><li><strong>Pattern:</strong> Store <strong>role/project/team</strong> (or resource IDs) as metadata on chunks; at query time, filter by <code>user.roles</code> / <code>user.projects</code> before or after vector search.</li><li><strong>Post-query filtering:</strong> Run permission checks on retrieved doc IDs against your auth system to avoid leaking snippets from unauthorized runbooks or repos.</li></ul></li><li><strong>Sensitive data:</strong> Redact PII/credentials in runbooks and Jira exports <strong>before</strong> embedding (e.g. Comprehend-style or regex + allowlists). Prefer “redact at ingest” so sensitive text never enters the vector DB.</li><li><strong>Network:</strong> Run RAG stack (embeddings, vector DB, LLM) in a private VPC; expose only the RAG API (and optional web UI) through an auth gateway.</li><li><strong>Audit:</strong> Log queries and which documents were retrieved (doc IDs, not full content) for compliance and debugging.</li></ul><hr><h2 id="_5-integration-options" tabindex="-1">5. Integration options <a class="header-anchor" href="#_5-integration-options" aria-label="Permalink to &quot;5. Integration options&quot;">​</a></h2><table tabindex="0"><thead><tr><th>Integration</th><th>Use case</th><th>Effort</th><th>Notes</th></tr></thead><tbody><tr><td><strong>Web UI</strong></td><td>Primary interface for runbooks, Jira, infra docs</td><td>Medium</td><td>Simple chat UI + source citations; auth via existing SSO. Best for POC.</td></tr><tr><td><strong>Slack</strong></td><td>“How do we…?” in channels or DMs</td><td>Medium</td><td>Bot with RAG backend; use Slack OAuth + scopes; optional RAGTime-style patterns (mention-based, threads).</td></tr><tr><td><strong>VSCode / Cursor</strong></td><td>Code-aware Q&amp;A in editor</td><td>Higher</td><td>Extensions like <strong>RAGnarōk</strong> (local embeddings + LanceDB) or custom extension calling your RAG API; good for “how does plugin X work?” with code context.</td></tr><tr><td><strong>API-only</strong></td><td>Other tools (e.g. internal dashboards, CLI)</td><td>Low</td><td>REST/GraphQL endpoint: <code>query</code> + <code>user_id</code>/<code>roles</code>; return answer + doc refs.</td></tr></tbody></table><p><strong>POC priority:</strong> Web UI first (proves value, easy to demo), then Slack; VSCode once retrieval quality is validated.</p><hr><h2 id="_6-estimated-infrastructure-costs" tabindex="-1">6. Estimated infrastructure costs <a class="header-anchor" href="#_6-estimated-infrastructure-costs" aria-label="Permalink to &quot;6. Estimated infrastructure costs&quot;">​</a></h2><p>Rough <strong>monthly</strong> ranges for a <strong>private</strong> stack (single region, no managed Pinecone/OpenAI for core RAG).</p><table tabindex="0"><thead><tr><th>Component</th><th>Low (POC / small team)</th><th>Medium (10–20 devs)</th><th>High (50+ devs, 10M+ chunks)</th></tr></thead><tbody><tr><td><strong>Embeddings</strong></td><td>Self-hosted (e5-base) on 1× CPU or small GPU: ~$50–100</td><td>1× L4 or T4 GPU: ~$200–400</td><td>Dedicated embedding tier: ~$500–1,500</td></tr><tr><td><strong>Vector DB</strong></td><td>Chroma/Qdrant single node: ~$60–120</td><td>Qdrant/Weaviate cluster: ~$300–600</td><td>~$600–1,000</td></tr><tr><td><strong>LLM inference</strong></td><td>Ollama on existing server or 1× small GPU: ~$0–150</td><td>1× A10/L4: ~$300–600</td><td>Multiple GPUs or managed: ~$1,000–3,000</td></tr><tr><td><strong>Reranker (optional)</strong></td><td>Omit or CPU: ~$0</td><td>1× small GPU: ~$100–200</td><td>~$200–500</td></tr><tr><td><strong>App + API</strong></td><td>1× small VM: ~$20–50</td><td>2–3 VMs + LB: ~$100–200</td><td>~$200–400</td></tr><tr><td><strong>Total</strong></td><td><strong>~$130–420/mo</strong></td><td><strong>~$1,000–2,000/mo</strong></td><td><strong>~$2,500–6,400/mo</strong></td></tr></tbody></table><ul><li><strong>Ingestion:</strong> One-time or batched; cost is dominated by embedding compute. 1M docs with self-hosted e5: order of hours on a single GPU, not ongoing API spend.</li><li><strong>Managed alternatives:</strong> If you later use OpenAI for embeddings (~$0.02/1M tokens), 1M docs ≈ tens of dollars one-time; 100M docs can reach ~$2k. LLM APIs (GPT-4, etc.) can push total to <strong>$50–100/day</strong> at high query volume if unoptimized.</li></ul><hr><h2 id="_7-architecture-overview" tabindex="-1">7. Architecture overview <a class="header-anchor" href="#_7-architecture-overview" aria-label="Permalink to &quot;7. Architecture overview&quot;">​</a></h2><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>┌─────────────────────────────────────────────────────────────────────────────┐</span></span>
<span class="line"><span>│                           DATA SOURCES                                       │</span></span>
<span class="line"><span>│  Runbooks │ Jira (CSV/JSON/export) │ Infra docs │ Plugin docs │ Internal MD  │</span></span>
<span class="line"><span>└───────────────────────────────────┬─────────────────────────────────────────┘</span></span>
<span class="line"><span>                                    │</span></span>
<span class="line"><span>                                    ▼</span></span>
<span class="line"><span>┌─────────────────────────────────────────────────────────────────────────────┐</span></span>
<span class="line"><span>│  INGEST PIPELINE                                                             │</span></span>
<span class="line"><span>│  • Parsers (Markdown, HTML, Jira schema, code repos)                         │</span></span>
<span class="line"><span>│  • Chunking: semantic (docs) + AST (code, Tree-sitter)                       │</span></span>
<span class="line"><span>│  • Metadata: source, type, project, access_roles                             │</span></span>
<span class="line"><span>│  • Redaction (PII/creds) → Embedding model (e5/BGE) → Vector DB (Qdrant/…)   │</span></span>
<span class="line"><span>└───────────────────────────────────┬─────────────────────────────────────────┘</span></span>
<span class="line"><span>                                    │</span></span>
<span class="line"><span>                                    ▼</span></span>
<span class="line"><span>┌─────────────────────────────────────────────────────────────────────────────┐</span></span>
<span class="line"><span>│  VECTOR DB                    │  AUTH / RBAC                                │</span></span>
<span class="line"><span>│  Collections by type/project  │  User ↔ roles, projects                      │</span></span>
<span class="line"><span>└───────────────────────────────────┬─────────────────────────────────────────┘</span></span>
<span class="line"><span>                                    │</span></span>
<span class="line"><span>                                    ▼</span></span>
<span class="line"><span>┌─────────────────────────────────────────────────────────────────────────────┐</span></span>
<span class="line"><span>│  RAG SERVICE (API)                                                            │</span></span>
<span class="line"><span>│  Query → embed → vector search + metadata filter (RBAC) → rerank → LLM       │</span></span>
<span class="line"><span>│  Response + cited doc IDs/snippets                                            │</span></span>
<span class="line"><span>└───────────────────────────────────┬─────────────────────────────────────────┘</span></span>
<span class="line"><span>                                    │</span></span>
<span class="line"><span>        ┌───────────────────────────┼───────────────────────────┐</span></span>
<span class="line"><span>        ▼                           ▼                           ▼</span></span>
<span class="line"><span>┌───────────────┐         ┌─────────────────┐         ┌─────────────────┐</span></span>
<span class="line"><span>│   Web UI      │         │  Slack bot       │         │  VSCode / API   │</span></span>
<span class="line"><span>│   (primary)   │         │  (optional)     │         │  (optional)     │</span></span>
<span class="line"><span>└───────────────┘         └─────────────────┘         └─────────────────┘</span></span></code></pre></div><ul><li><strong>Private:</strong> Embedding model, vector DB, and LLM run in your VPC (or on-prem). No third-party RAG SaaS in the critical path.</li><li><strong>Access:</strong> Every query is bound to an authenticated user; vector search or post-retrieval filter enforces RBAC so only allowed runbooks/docs/code are used in the answer.</li></ul><hr><h2 id="_8-60-day-poc-plan" tabindex="-1">8. 60-day POC plan <a class="header-anchor" href="#_8-60-day-poc-plan" aria-label="Permalink to &quot;8. 60-day POC plan&quot;">​</a></h2><table tabindex="0"><thead><tr><th>Phase</th><th>Week</th><th>Activities</th><th>Deliverables</th></tr></thead><tbody><tr><td><strong>Setup</strong></td><td>1–2</td><td>Pick stack: Ollama or Python + e5/BGE; Qdrant or Chroma in Docker. Ingest 2–3 runbooks + one plugin’s README/code.</td><td>Running pipeline; vector DB with ~500–2k chunks.</td></tr><tr><td><strong>Chunking</strong></td><td>2–3</td><td>Implement semantic chunking for Markdown/runbooks; AST (Tree-sitter) for one repo (e.g. PHP plugin). Tag <code>type</code>, <code>source</code>, <code>project</code>.</td><td>Two ingest paths; metadata in vector DB.</td></tr><tr><td><strong>RAG API</strong></td><td>3–4</td><td>Query API: embed query → search → filter by test user/role → call local LLM (Ollama) → return answer + citations.</td><td>REST or GraphQL endpoint; curl-able.</td></tr><tr><td><strong>Auth</strong></td><td>4–5</td><td>Wire SSO (or mock roles); apply role/project filter in retrieval; audit log (query + doc IDs).</td><td>Only allowed docs appear in answers.</td></tr><tr><td><strong>Web UI</strong></td><td>5–6</td><td>Simple chat UI (e.g. Streamlit, React, or Vite + your API); show sources per message.</td><td>Demo-able “internal assistant”.</td></tr><tr><td><strong>Content</strong></td><td>6–7</td><td>Ingest Jira export (e.g. epic/issue summaries + links); infra doc set; 2–3 internal Markdown hubs.</td><td>Broader coverage; ~10k–50k chunks.</td></tr><tr><td><strong>Eval</strong></td><td>7–8</td><td>Define 30–50 test questions; measure retrieval (Recall@5, MRR) and answer quality (faithfulness, relevance). Tune chunk size/overlap and model if needed.</td><td>Eval report; decision to scale or iterate.</td></tr></tbody></table><p><strong>Success criteria for POC:</strong> (1) Answers grounded in your runbooks/docs with citations. (2) No retrieval from docs the test user is not allowed to see. (3) Latency &lt; 5 s for typical query on single-node setup.</p><hr><h2 id="_9-evaluation-metrics" tabindex="-1">9. Evaluation metrics <a class="header-anchor" href="#_9-evaluation-metrics" aria-label="Permalink to &quot;9. Evaluation metrics&quot;">​</a></h2><table tabindex="0"><thead><tr><th>Category</th><th>Metrics</th><th>How to measure</th></tr></thead><tbody><tr><td><strong>Retrieval</strong></td><td><strong>Recall@5</strong>, <strong>MRR</strong></td><td>For each test question, check if the gold doc(s) appear in top 5; compute recall and mean reciprocal rank.</td></tr><tr><td><strong>Faithfulness</strong></td><td>% of claims supported by retrieved chunks</td><td>LLM-as-judge or NLI model: “Does this passage support this claim?” — avoid hallucinations.</td></tr><tr><td><strong>Relevance</strong></td><td>Semantic match query ↔ answer</td><td>LLM-as-judge or embedding similarity (query vs answer).</td></tr><tr><td><strong>Citation</strong></td><td>Correctness of cited doc IDs</td><td>Whether cited chunks actually support the sentence they’re attached to.</td></tr><tr><td><strong>Safety / RBAC</strong></td><td>Leak rate</td><td>For each role, run queries that <em>should</em> not see certain docs; confirm those docs never appear in top-k or in citations.</td></tr></tbody></table><p><strong>Tools:</strong> <strong>Ragas</strong> (reference-free), <strong>DeepEval</strong> (RAG metrics out of the box), or custom scripts with your test Q set. Track these weekly during POC and set targets (e.g. Recall@5 &gt; 0.85, zero RBAC leaks).</p><hr><h2 id="_10-summary-and-next-steps" tabindex="-1">10. Summary and next steps <a class="header-anchor" href="#_10-summary-and-next-steps" aria-label="Permalink to &quot;10. Summary and next steps&quot;">​</a></h2><ul><li><strong>Embeddings:</strong> e5-base or BGE-base for POC; self-host via Ollama or Sentence Transformers.</li><li><strong>Vector DB:</strong> Qdrant (Docker) for performance and simplicity; Chroma for quickest POC.</li><li><strong>Chunking:</strong> Semantic for runbooks/docs/Jira/Markdown; AST (Tree-sitter) for WordPress/Laravel code.</li><li><strong>Security:</strong> SSO + RBAC on retrieval; redact sensitive data at ingest; private VPC and audit logging.</li><li><strong>Integrations:</strong> Web UI first, then Slack; VSCode/API once stable.</li><li><strong>Cost:</strong> POC ~$130–420/mo; production small team ~$1k–2k/mo depending on LLM and scale.</li></ul><p><strong>Next steps:</strong> (1) Choose one runbook and one codebase as Day-1 sources. (2) Stand up vector DB + embedding model in Docker. (3) Implement ingest with semantic + AST chunking and RBAC metadata. (4) Build minimal RAG API and web UI and run a 30-question eval by end of Week 8.</p>`,49)])])}const h=e(a,[["render",o]]);export{u as __pageData,h as default};
