import{_ as e,o as a,c as r,a0 as o}from"./chunks/framework.B3sz4m_N.js";const h=JSON.parse('{"title":"AI in the WordPress + Laravel Development Pipeline","description":"Research on structural AI integration beyond code completion — PR review, refactoring, migration, testing, documentation, tooling, security, and a 90-day pilot.","frontmatter":{"title":"AI in the WordPress + Laravel Development Pipeline","description":"Research on structural AI integration beyond code completion — PR review, refactoring, migration, testing, documentation, tooling, security, and a 90-day pilot."},"headers":[],"relativePath":"reports/ai-wp-laravel-pipeline/01-research-document.md","filePath":"reports/ai-wp-laravel-pipeline/01-research-document.md"}'),n={name:"reports/ai-wp-laravel-pipeline/01-research-document.md"};function i(s,t,d,l,c,u){return a(),r("div",null,[...t[0]||(t[0]=[o('<h1 id="ai-integration-in-wordpress-laravel-development-pipelines" tabindex="-1">AI Integration in WordPress + Laravel Development Pipelines <a class="header-anchor" href="#ai-integration-in-wordpress-laravel-development-pipelines" aria-label="Permalink to &quot;AI Integration in WordPress + Laravel Development Pipelines&quot;">​</a></h1><p><strong>Research document — structural integration beyond code completion</strong></p><p><em>Context: Large legacy WordPress (ACF, Classic Editor), Laravel (queues, APIs, Passport), client retainer work, PHP 7.4→8.x transition, GitHub + deployment CI/CD.</em></p><hr><h2 id="_1-key-opportunities-for-ai-augmentation" tabindex="-1">1. Key Opportunities for AI Augmentation <a class="header-anchor" href="#_1-key-opportunities-for-ai-augmentation" aria-label="Permalink to &quot;1. Key Opportunities for AI Augmentation&quot;">​</a></h2><h3 id="_1-1-pr-review" tabindex="-1">1.1 PR Review <a class="header-anchor" href="#_1-1-pr-review" aria-label="Permalink to &quot;1.1 PR Review&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Opportunity</th><th>Description</th><th>Fit for stack</th></tr></thead><tbody><tr><td><strong>Automated first-pass review</strong></td><td>AI summarises PR scope, suggests missing tests/docs, flags obvious bugs before human review.</td><td>High — reduces bottleneck on senior reviewers; works across PHP (WordPress/Laravel) and JS.</td></tr><tr><td><strong>Policy and standard checks</strong></td><td>Enforce naming, escaping, nonce usage, capability checks (WP) and auth/validation patterns (Laravel).</td><td>High — ACF/legacy WP and Passport APIs benefit from consistent security checks.</td></tr><tr><td><strong>Context-aware suggestions</strong></td><td>Use repo rules (e.g. Cursor/AGENTS.md) so suggestions align with Classic Editor vs Gutenberg, queue usage, API versioning.</td><td>Medium — requires curating rules per project.</td></tr><tr><td><strong>Reviewer routing</strong></td><td>Suggest assignees by file/module (WP theme vs Laravel app vs shared lib).</td><td>Medium — useful for retainer teams with mixed ownership.</td></tr></tbody></table><p><strong>Actionable next steps</strong></p><ul><li>Add an AI PR review step in GitHub Actions (e.g. CodeRabbit, Graphite Agent, or Greptile) on a single repo for a 4-week trial.</li><li>Define a short “PR review policy” doc (security, sanitisation, tests) and feed it into the tool’s context where possible.</li><li>Compare: time to first review, number of issues found in review vs post-merge, and reviewer satisfaction (survey).</li></ul><hr><h3 id="_1-2-refactoring" tabindex="-1">1.2 Refactoring <a class="header-anchor" href="#_1-2-refactoring" aria-label="Permalink to &quot;1.2 Refactoring&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Opportunity</th><th>Description</th><th>Fit for stack</th></tr></thead><tbody><tr><td><strong>Rector (deterministic)</strong></td><td>Rule-based PHP refactors and version upgrades; no LLM. Handles renames, type additions, deprecated API replacements.</td><td><strong>Primary</strong> — use Rector for PHP 7.4→8.x and framework upgrades.</td></tr><tr><td><strong>AI-assisted extraction</strong></td><td>Suggest extracting ACF-heavy logic into service classes or moving inline SQL to Eloquent/query builders.</td><td>High — for legacy WP and Laravel; always human-verify.</td></tr><tr><td><strong>Smell and pattern detection</strong></td><td>SonarQube/Psalm/PHPStan find issues; AI suggests refactor direction (e.g. “replace global with dependency injection”).</td><td>High — combine static analysis with AI “what to do” suggestions.</td></tr><tr><td><strong>Incremental scope</strong></td><td>Refactor by file or by feature (single ACF block, single API module) to keep changes reviewable.</td><td>Critical — avoid “refactor entire theme” prompts.</td></tr></tbody></table><p><strong>Actionable next steps</strong></p><ul><li>Introduce Rector with a narrow rule set (e.g. <code>PHP74*</code>, <code>DeadCode</code>, one WP/Laravel rule set) in a branch; measure time saved vs manual upgrade.</li><li>Use AI (Cursor/Copilot/PhpStorm AI) for <em>suggestions</em> only: “suggest a refactor for this function” → apply and test manually.</li><li>Document “refactor playbook”: run Rector → run PHPStan baseline → AI-suggest refactors for top N files → PR + QA.</li></ul><hr><h3 id="_1-3-migration-php-wordpress-upgrades" tabindex="-1">1.3 Migration (PHP + WordPress upgrades) <a class="header-anchor" href="#_1-3-migration-php-wordpress-upgrades" aria-label="Permalink to &quot;1.3 Migration (PHP + WordPress upgrades)&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Opportunity</th><th>Description</th><th>Fit for stack</th></tr></thead><tbody><tr><td><strong>PHP 7.4 → 8.x</strong></td><td>Rector automates most syntax and BC breaks (typed properties, match, nullsafe, deprecations). PHPStan/Psalm catch remaining type issues.</td><td><strong>Primary</strong> — proven; use baselines for legacy code.</td></tr><tr><td><strong>WordPress / ACF upgrades</strong></td><td>No single “AI migrator”; use Rector for PHP, then manual + AI-assisted updates for template and ACF API changes.</td><td>Medium — AI can draft upgrade notes and patch examples.</td></tr><tr><td><strong>Dependency upgrades</strong></td><td>Composer update + static analysis; AI can help resolve conflict suggestions and changelog summaries.</td><td>Medium — useful for retainer “keep dependencies current” work.</td></tr></tbody></table><p><strong>Actionable next steps</strong></p><ul><li>Create a migration pipeline: Rector (PHP 8.x rules) → PHPStan (level 0 or 1, baseline) → existing test suite. Run on a copy of one client codebase.</li><li>Use AI to generate short “migration runbooks” per project (known ACF/plugin quirks, manual steps).</li><li>Track: files changed, build/test pass rate, and rollback frequency.</li></ul><hr><h3 id="_1-4-test-generation" tabindex="-1">1.4 Test Generation <a class="header-anchor" href="#_1-4-test-generation" aria-label="Permalink to &quot;1.4 Test Generation&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Opportunity</th><th>Description</th><th>Fit for stack</th></tr></thead><tbody><tr><td><strong>Scaffolding</strong></td><td>AI generates PHPUnit/Codeception boilerplate from class/method signatures; developers fill in assertions and edge cases.</td><td>High — reduces “no tests because setup is tedious” (common in PHP surveys).</td></tr><tr><td><strong>From specs / tickets</strong></td><td>Given a scenario (e.g. “user cannot access other tenant’s data”), AI suggests test cases; team refines and implements.</td><td>High — aligns with spec-driven and client retainer work.</td></tr><tr><td><strong>Integration tests</strong></td><td>AI drafts HTTP tests for Laravel APIs (Passport auth, queues) or WP REST endpoints; team adds fixtures and assertions.</td><td>Medium — verify auth and side effects manually.</td></tr><tr><td><strong>Regression tests</strong></td><td>After a bug fix, “generate a test that would have caught this” → human edits and commits.</td><td>High — improves coverage where it matters most.</td></tr></tbody></table><p><strong>Actionable next steps</strong></p><ul><li>Standardise on PHPUnit (and optionally Codeception for WP). Use PhpStorm AI / Cursor / Copilot to generate test <em>scaffolds</em> from selected code; never auto-commit without review.</li><li>Add a “test suggestion” step: when closing a bug ticket, prompt “suggest a PHPUnit test for this fix” and add to Definition of Done.</li><li>Measure: new tests per sprint, failure catch rate (bugs found in CI vs production).</li></ul><hr><h3 id="_1-5-documentation" tabindex="-1">1.5 Documentation <a class="header-anchor" href="#_1-5-documentation" aria-label="Permalink to &quot;1.5 Documentation&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Opportunity</th><th>Description</th><th>Fit for stack</th></tr></thead><tbody><tr><td><strong>PHPDoc / inline docs</strong></td><td>AI generates param/return/throws blocks from signatures and usage; maintainers correct and extend.</td><td>High — improves IDE support and onboarding.</td></tr><tr><td><strong>API docs</strong></td><td>From Laravel routes and controllers (or WP REST registration), generate OpenAPI/Postman-friendly docs; AI can summarise and add examples.</td><td>High — Passport and custom endpoints benefit.</td></tr><tr><td><strong>Runbooks and ADRs</strong></td><td>AI drafts runbooks (deploy, rollback, env vars) and ADRs from commit/ticket history; team approves and owns.</td><td>Medium — good for retainer handover and new joiners.</td></tr><tr><td><strong>Client-facing specs</strong></td><td>Keep specs human-written; use AI to check consistency (e.g. “does this spec match the acceptance criteria in the ticket?”).</td><td>Medium — reduces drift between spec and implementation.</td></tr></tbody></table><p><strong>Actionable next steps</strong></p><ul><li>Add a “documentation pass” in the pipeline: e.g. on merge to main, run a job that suggests PHPDoc for changed PHP files (via AI or Workik-style tool); output as comment or PR.</li><li>Use AI to generate first draft of API docs from route definitions; maintain as code-first (e.g. OpenAPI from annotations).</li><li>Define “documentation ownership”: who approves AI-generated runbooks/ADRs before they become canonical.</li></ul><hr><h2 id="_2-tool-comparison-open-source-vs-saas" tabindex="-1">2. Tool Comparison (Open-Source vs SaaS) <a class="header-anchor" href="#_2-tool-comparison-open-source-vs-saas" aria-label="Permalink to &quot;2. Tool Comparison (Open-Source vs SaaS)&quot;">​</a></h2><h3 id="_2-1-pr-code-review" tabindex="-1">2.1 PR / Code Review <a class="header-anchor" href="#_2-1-pr-code-review" aria-label="Permalink to &quot;2.1 PR / Code Review&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Tool</th><th>Type</th><th>Strengths</th><th>Limitations</th><th>Cost note</th></tr></thead><tbody><tr><td><strong>SonarQube</strong></td><td>OSS (CE) / Commercial</td><td>Quality gates, security, multi-language, CI integration</td><td>Limited “context” review; rule tuning required</td><td>CE free; commercial for advanced rules</td></tr><tr><td><strong>Semgrep</strong></td><td>OSS / SaaS</td><td>Security-focused, autofix, custom rules</td><td>PHP support good but not WP-specific</td><td>OSS free; Team/Enterprise paid</td></tr><tr><td><strong>CodeRabbit</strong></td><td>SaaS</td><td>AI PR comments, GitHub-native</td><td>Variable catch rate (benchmarks ~44% bug catch)</td><td>Free tier; paid per repo/seat</td></tr><tr><td><strong>Greptile</strong></td><td>SaaS</td><td>High benchmark bug catch (~82%)</td><td>SaaS only; data leaves your control</td><td>Paid</td></tr><tr><td><strong>Graphite Agent</strong></td><td>SaaS</td><td>AI review, free tier (e.g. 100 PRs/month)</td><td>Feature set vs others TBD</td><td>Free tier available</td></tr><tr><td><strong>Cursor BugBot</strong></td><td>SaaS</td><td>Integrated with Cursor workflow</td><td>Cursor-centric; ~58% catch in benchmarks</td><td>Part of Cursor</td></tr></tbody></table><p><strong>Recommendation</strong></p><ul><li><strong>Hybrid</strong>: SonarQube or Semgrep in CI for deterministic quality/security; one AI review tool (e.g. CodeRabbit or Graphite Agent) for first-pass PR review. Prefer a tool that allows custom context (policy doc, repo rules).</li></ul><hr><h3 id="_2-2-refactoring-migration" tabindex="-1">2.2 Refactoring &amp; Migration <a class="header-anchor" href="#_2-2-refactoring-migration" aria-label="Permalink to &quot;2.2 Refactoring &amp; Migration&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Tool</th><th>Type</th><th>Strengths</th><th>Limitations</th><th>Cost note</th></tr></thead><tbody><tr><td><strong>Rector</strong></td><td>OSS</td><td>PHP 7.4→8.x, framework rules, no LLM dependency</td><td>Rule sets need choosing; some edge cases manual</td><td>Free (OSS); paid support available</td></tr><tr><td><strong>PHPStan</strong></td><td>OSS</td><td>Static analysis, baselines, WordPress/Laravel extensions</td><td>No auto-fix; suggests only</td><td>Free</td></tr><tr><td><strong>Psalm</strong></td><td>OSS</td><td>Types, security, auto-fix for some issues</td><td>Different ecosystem from PHPStan</td><td>Free</td></tr><tr><td><strong>CodePorting (PHP)</strong></td><td>SaaS</td><td>AI-driven syntax modernisation</td><td>Less proven than Rector for PHP upgrades</td><td>Commercial</td></tr><tr><td><strong>PhpStorm + AI</strong></td><td>Commercial</td><td>Refactor + AI in IDE</td><td>Per-seat cost; tied to JetBrains</td><td>Subscription</td></tr></tbody></table><p><strong>Recommendation</strong></p><ul><li><strong>Rector</strong> as the backbone for PHP and framework upgrades. <strong>PHPStan</strong> (or Psalm) for continuous static analysis with baseline for legacy. Use <strong>AI (Cursor/Copilot/PhpStorm)</strong> for <em>suggestions</em> and drafting, not as the single source of truth for migrations.</li></ul><hr><h3 id="_2-3-test-generation" tabindex="-1">2.3 Test Generation <a class="header-anchor" href="#_2-3-test-generation" aria-label="Permalink to &quot;2.3 Test Generation&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Tool</th><th>Type</th><th>Strengths</th><th>Limitations</th><th>Cost note</th></tr></thead><tbody><tr><td><strong>PhpStorm AI Assistant</strong></td><td>Commercial</td><td>PHPUnit scaffolding, TDD flow</td><td>IDE-bound; subscription</td><td>JetBrains subscription</td></tr><tr><td><strong>Cursor / Copilot</strong></td><td>SaaS</td><td>In-editor test generation from selection</td><td>Quality variable; must review</td><td>Per-seat</td></tr><tr><td><strong>Codeception</strong></td><td>OSS</td><td>Acceptance/API tests, WP integration</td><td>No AI; you write tests</td><td>Free</td></tr></tbody></table><p><strong>Recommendation</strong></p><ul><li>Use <strong>AI for scaffolding and suggestions</strong> (PhpStorm AI or Cursor/Copilot); <strong>PHPUnit + Codeception</strong> as the actual test stack. Never auto-commit AI-generated tests without running them and a quick review.</li></ul><hr><h3 id="_2-4-documentation" tabindex="-1">2.4 Documentation <a class="header-anchor" href="#_2-4-documentation" aria-label="Permalink to &quot;2.4 Documentation&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Tool</th><th>Type</th><th>Strengths</th><th>Limitations</th><th>Cost note</th></tr></thead><tbody><tr><td><strong>Workik (PHP)</strong></td><td>SaaS</td><td>PHPDoc, API docs, multi-framework</td><td>Upload/code access model</td><td>Free tier cited</td></tr><tr><td><strong>WP LLM / MCP for WP</strong></td><td>OSS / Integrations</td><td>WordPress-aware, REST tooling</td><td>More for generation than doc pipeline</td><td>Varies</td></tr><tr><td><strong>Generic LLM (Claude/ChatGPT)</strong></td><td>SaaS</td><td>Flexible drafts for runbooks, ADRs</td><td>No direct codebase link without RAG</td><td>Per use</td></tr></tbody></table><p><strong>Recommendation</strong></p><ul><li>Use <strong>AI to draft</strong> PHPDoc and API descriptions; keep <strong>source of truth in repo</strong> (e.g. OpenAPI from code). For runbooks/ADRs, use AI as draft generator with human ownership.</li></ul><hr><h2 id="_3-security-and-governance-considerations" tabindex="-1">3. Security and Governance Considerations <a class="header-anchor" href="#_3-security-and-governance-considerations" aria-label="Permalink to &quot;3. Security and Governance Considerations&quot;">​</a></h2><h3 id="_3-1-data-and-code-exposure" tabindex="-1">3.1 Data and code exposure <a class="header-anchor" href="#_3-1-data-and-code-exposure" aria-label="Permalink to &quot;3.1 Data and code exposure&quot;">​</a></h3><ul><li><strong>SaaS AI (Copilot, CodeRabbit, Greptile, etc.)</strong>: Code and comments may be sent to third-party APIs. Check terms for training use and retention.</li><li><strong>Mitigation</strong>: Prefer “no training on my data” options; use private repos and minimal necessary context; for high-sensitivity clients, prefer on-prem or OSS-first (SonarQube, Semgrep, Rector, PHPStan) where possible.</li></ul><h3 id="_3-2-supply-chain-and-hallucination" tabindex="-1">3.2 Supply chain and hallucination <a class="header-anchor" href="#_3-2-supply-chain-and-hallucination" aria-label="Permalink to &quot;3.2 Supply chain and hallucination&quot;">​</a></h3><ul><li><strong>Hallucinations</strong>: AI can suggest non-existent APIs, packages, or WP/Laravel APIs. Can lead to “slopsquatting” if developers install packages that match hallucinated names.</li><li><strong>Mitigation</strong>: (1) Always run tests and static analysis (PHPStan/Rector) after applying AI suggestions. (2) Prefer deterministic tools (Rector, PHPStan) for migrations and structural changes. (3) Short checklist: “Did we run the test suite? Did we verify this function/package exists?”</li></ul><h3 id="_3-3-policy-and-compliance" tabindex="-1">3.3 Policy and compliance <a class="header-anchor" href="#_3-3-policy-and-compliance" aria-label="Permalink to &quot;3.3 Policy and compliance&quot;">​</a></h3><ul><li><strong>AI usage policy</strong>: Define what is allowed (e.g. PR review, test scaffolding, doc drafts) and what is not (e.g. auto-merge, production config generation without review). Document in a short “AI use” page and link from repo.</li><li><strong>Attribution and liability</strong>: Clarify that AI output is not legal/contractual advice; client-facing deliverables remain human-approved. For retainer work, ensure contracts or SOWs allow use of approved AI tools.</li></ul><h3 id="_3-4-access-and-audit" tabindex="-1">3.4 Access and audit <a class="header-anchor" href="#_3-4-access-and-audit" aria-label="Permalink to &quot;3.4 Access and audit&quot;">​</a></h3><ul><li><strong>Who can enable what</strong>: Decide which AI tools are allowed per repo/environment (e.g. GitHub integration only for certain orgs). Use GitHub (or IdP) permissions so only authorised users trigger AI steps.</li><li><strong>Audit</strong>: Log where AI was used (e.g. “PR comments by CodeRabbit”) so you can trace back if an issue is attributed to an AI suggestion.</li></ul><p><strong>Actionable next steps</strong></p><ul><li>Draft a one-page “AI in our pipeline” policy (data, tools, review requirements) and get sign-off from leadership/client where needed.</li><li>Prefer tools with clear “no training on your data” and retention policies; document chosen tools and data flows.</li><li>Add a lightweight audit: e.g. label or comment in PRs when an AI tool was used for review or suggestions.</li></ul><hr><h2 id="_4-90-day-pilot-implementation-roadmap" tabindex="-1">4. 90-Day Pilot Implementation Roadmap <a class="header-anchor" href="#_4-90-day-pilot-implementation-roadmap" aria-label="Permalink to &quot;4. 90-Day Pilot Implementation Roadmap&quot;">​</a></h2><h3 id="phase-1-foundation-days-1–30" tabindex="-1">Phase 1: Foundation (Days 1–30) <a class="header-anchor" href="#phase-1-foundation-days-1–30" aria-label="Permalink to &quot;Phase 1: Foundation (Days 1–30)&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Week</th><th>Focus</th><th>Actions</th><th>Owner</th></tr></thead><tbody><tr><td>1</td><td>Scope and policy</td><td>Choose one WordPress and one Laravel repo for pilot. Draft AI usage policy and get approval.</td><td>Tech lead / PM</td></tr><tr><td>2</td><td>CI and static analysis</td><td>Ensure PHPStan (or Psalm) and Rector run in CI; add or update baseline for legacy code.</td><td>Dev</td></tr><tr><td>3</td><td>PR review tool</td><td>Enable one AI PR review tool (e.g. CodeRabbit or Graphite Agent) on pilot repos; add link to “PR review expectations” doc.</td><td>Dev</td></tr><tr><td>4</td><td>Baseline metrics</td><td>Measure: PR cycle time, time to first review, test coverage, and one “developer satisfaction” pulse.</td><td>PM / Tech lead</td></tr></tbody></table><p><strong>Exit criteria</strong>: CI green with PHPStan + Rector; AI PR review active; baseline metrics recorded.</p><hr><h3 id="phase-2-refactor-and-migration-days-31–60" tabindex="-1">Phase 2: Refactor and migration (Days 31–60) <a class="header-anchor" href="#phase-2-refactor-and-migration-days-31–60" aria-label="Permalink to &quot;Phase 2: Refactor and migration (Days 31–60)&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Week</th><th>Focus</th><th>Actions</th><th>Owner</th></tr></thead><tbody><tr><td>5</td><td>Rector migration branch</td><td>Create branch on one pilot repo; run Rector with PHP 8.x rule set; fix remaining issues with PHPStan; document manual steps.</td><td>Dev</td></tr><tr><td>6</td><td>Test scaffolding</td><td>Run a “test gen” experiment: use AI to generate PHPUnit scaffolds for 5–10 high-value classes; run and fix; measure time vs writing from scratch.</td><td>Dev</td></tr><tr><td>7</td><td>Documentation pass</td><td>Use AI to generate PHPDoc for one module (e.g. one Laravel API or one WP plugin); review and merge; document process.</td><td>Dev</td></tr><tr><td>8</td><td>Mid-pilot review</td><td>Compare metrics to baseline; gather developer feedback; decide whether to expand or adjust tools.</td><td>Tech lead / PM</td></tr></tbody></table><p><strong>Exit criteria</strong>: One Rector upgrade path documented; test and doc experiments completed; mid-pilot report.</p><hr><h3 id="phase-3-scale-and-embed-days-61–90" tabindex="-1">Phase 3: Scale and embed (Days 61–90) <a class="header-anchor" href="#phase-3-scale-and-embed-days-61–90" aria-label="Permalink to &quot;Phase 3: Scale and embed (Days 61–90)&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Week</th><th>Focus</th><th>Actions</th><th>Owner</th></tr></thead><tbody><tr><td>9</td><td>Expand PR review</td><td>Roll out AI PR review to remaining pilot repos (or all retainer repos if small). Tune rules/policy context.</td><td>Dev</td></tr><tr><td>10</td><td>Migration rollout</td><td>Apply Rector-driven PHP 8.x upgrade on one non-pilot client (or staging) using playbook from Phase 2.</td><td>Dev</td></tr><tr><td>11</td><td>Playbooks and training</td><td>Finalise “AI in our pipeline” playbook (when to use Rector, when to use AI for tests/docs, review requirements). Short internal training or doc share.</td><td>Tech lead</td></tr><tr><td>12</td><td>Report and KPIs</td><td>Publish pilot report: metrics, lessons, risks, and recommendations. Define ongoing KPIs and who owns them.</td><td>PM / Tech lead</td></tr></tbody></table><p><strong>Exit criteria</strong>: Playbook published; at least one full migration using Rector; final report with KPIs and next steps.</p><hr><h2 id="_5-kpis-to-measure-productivity-gain" tabindex="-1">5. KPIs to Measure Productivity Gain <a class="header-anchor" href="#_5-kpis-to-measure-productivity-gain" aria-label="Permalink to &quot;5. KPIs to Measure Productivity Gain&quot;">​</a></h2><h3 id="_5-1-delivery-and-quality" tabindex="-1">5.1 Delivery and quality <a class="header-anchor" href="#_5-1-delivery-and-quality" aria-label="Permalink to &quot;5.1 Delivery and quality&quot;">​</a></h3><table tabindex="0"><thead><tr><th>KPI</th><th>Definition</th><th>Target (example)</th><th>How to measure</th></tr></thead><tbody><tr><td><strong>PR cycle time</strong></td><td>Time from PR open to merge</td><td>−15–20% vs baseline</td><td>GitHub API or project management tool</td></tr><tr><td><strong>Time to first review</strong></td><td>Time from open to first review comment</td><td>−25%</td><td>GitHub events</td></tr><tr><td><strong>Escaped defects</strong></td><td>Bugs found in production post-release</td><td>No increase (or decrease)</td><td>Issue tracker, severity tagging</td></tr><tr><td><strong>Test coverage (critical paths)</strong></td><td>% coverage for chosen modules</td><td>+5–10% where targeted</td><td>PHPUnit/Codeception report</td></tr></tbody></table><h3 id="_5-2-migration-and-refactor" tabindex="-1">5.2 Migration and refactor <a class="header-anchor" href="#_5-2-migration-and-refactor" aria-label="Permalink to &quot;5.2 Migration and refactor&quot;">​</a></h3><table tabindex="0"><thead><tr><th>KPI</th><th>Definition</th><th>Target (example)</th><th>How to measure</th></tr></thead><tbody><tr><td><strong>Migration duration</strong></td><td>Calendar time for PHP 7.4→8.x per project</td><td>−50% vs prior estimate</td><td>Project timeline</td></tr><tr><td><strong>Rector auto-fix rate</strong></td><td>% of changes applied by Rector vs manual</td><td>Track and improve</td><td>Rector diff stats</td></tr><tr><td><strong>Post-migration rollbacks</strong></td><td>Rollbacks due to upgrade issues</td><td>Zero or minimal</td><td>Deploy/incident log</td></tr></tbody></table><h3 id="_5-3-adoption-and-trust" tabindex="-1">5.3 Adoption and trust <a class="header-anchor" href="#_5-3-adoption-and-trust" aria-label="Permalink to &quot;5.3 Adoption and trust&quot;">​</a></h3><table tabindex="0"><thead><tr><th>KPI</th><th>Definition</th><th>Target (example)</th><th>How to measure</th></tr></thead><tbody><tr><td><strong>AI suggestion acceptance</strong></td><td>% of AI PR suggestions accepted (or “addressed”)</td><td>Track; aim for steady or improving</td><td>Review tool analytics if available</td></tr><tr><td><strong>Developer satisfaction</strong></td><td>Short survey: “AI tools help me deliver” / “I trust AI suggestions after review”</td><td>Improve over pilot</td><td>Quarterly pulse (e.g. 1–5 scale)</td></tr><tr><td><strong>Documentation freshness</strong></td><td>% of touched modules with up-to-date PHPDoc/API docs</td><td>+20% over pilot</td><td>Sampling or automated checks</td></tr></tbody></table><p><strong>Actionable next steps</strong></p><ul><li>Define baseline for “PR cycle time” and “time to first review” in Week 4 of Phase 1.</li><li>Add one “productivity” question to existing retros or surveys (e.g. “How much did AI tooling help this sprint?”).</li><li>Review KPIs at end of Phase 2 and Phase 3; drop or add metrics based on what drives decisions.</li></ul><hr><h2 id="_6-risks-and-failure-modes" tabindex="-1">6. Risks and Failure Modes <a class="header-anchor" href="#_6-risks-and-failure-modes" aria-label="Permalink to &quot;6. Risks and Failure Modes&quot;">​</a></h2><h3 id="_6-1-over-reliance-on-ai" tabindex="-1">6.1 Over-reliance on AI <a class="header-anchor" href="#_6-1-over-reliance-on-ai" aria-label="Permalink to &quot;6.1 Over-reliance on AI&quot;">​</a></h3><ul><li><strong>Risk</strong>: Developers accept AI suggestions without verification; wrong or insecure code is merged.</li><li><strong>Mitigation</strong>: Mandate “all AI-suggested code must be run (tests + manual check) and reviewed”; keep Rector/PHPStan as gates. Include “verify AI suggestion” in PR checklist.</li></ul><h3 id="_6-2-hallucination-and-wrong-apis" tabindex="-1">6.2 Hallucination and wrong APIs <a class="header-anchor" href="#_6-2-hallucination-and-wrong-apis" aria-label="Permalink to &quot;6.2 Hallucination and wrong APIs&quot;">​</a></h3><ul><li><strong>Risk</strong>: AI invents WordPress/Laravel APIs or package names; builds break or security is compromised.</li><li><strong>Mitigation</strong>: Always run test suite and static analysis; prefer Rector for structural changes; short “AI checklist” (function exists? package official?).</li></ul><h3 id="_6-3-slows-down-review" tabindex="-1">6.3 Slows down review <a class="header-anchor" href="#_6-3-slows-down-review" aria-label="Permalink to &quot;6.3 Slows down review&quot;">​</a></h3><ul><li><strong>Risk</strong>: Too many low-value AI comments; reviewers ignore them or spend more time filtering than before.</li><li><strong>Mitigation</strong>: Tune AI review tool (e.g. severity, rule set); cap or categorise comments; regularly trim noisy rules.</li></ul><h3 id="_6-4-client-and-compliance-risk" tabindex="-1">6.4 Client and compliance risk <a class="header-anchor" href="#_6-4-client-and-compliance-risk" aria-label="Permalink to &quot;6.4 Client and compliance risk&quot;">​</a></h3><ul><li><strong>Risk</strong>: Client or contract forbids sending code to third-party AI; or data residency requires on-prem only.</li><li><strong>Mitigation</strong>: Confirm contract/SOW and client expectations before enabling SaaS AI; prefer OSS/on-prem (SonarQube, Semgrep, Rector, PHPStan) for sensitive clients.</li></ul><h3 id="_6-5-burnout-and-valuable-work" tabindex="-1">6.5 Burnout and “valuable work” <a class="header-anchor" href="#_6-5-burnout-and-valuable-work" aria-label="Permalink to &quot;6.5 Burnout and “valuable work”&quot;">​</a></h3><ul><li><strong>Risk</strong>: DORA-style finding: developers feel more “productive” but spend less time on work they find meaningful (e.g. design, deep problem-solving).</li><li><strong>Mitigation</strong>: Use AI for repetitive tasks (boilerplate, first-pass review, doc drafts); keep design and architecture decisions human-led. Survey “meaningful work” as well as “productivity.”</li></ul><h3 id="_6-6-pilot-fatigue" tabindex="-1">6.6 Pilot fatigue <a class="header-anchor" href="#_6-6-pilot-fatigue" aria-label="Permalink to &quot;6.6 Pilot fatigue&quot;">​</a></h3><ul><li><strong>Risk</strong>: Too many tools or steps added at once; team disengages and reverts to old workflow.</li><li><strong>Mitigation</strong>: Pilot with 1–2 repos and 2–3 tools max (e.g. PHPStan + Rector + one AI PR tool). Add test/doc AI only after PR review is stable. Celebrate small wins.</li></ul><p><strong>Actionable next steps</strong></p><ul><li>Add “Risks and mitigations” to the pilot charter; review in Week 4 and Week 8.</li><li>Maintain a “failure log”: when an AI suggestion caused a bug or rework, note it (anonymised) and adjust process or tool config.</li></ul><hr><h2 id="summary-and-next-steps" tabindex="-1">Summary and Next Steps <a class="header-anchor" href="#summary-and-next-steps" aria-label="Permalink to &quot;Summary and Next Steps&quot;">​</a></h2><ol><li><strong>Prioritise</strong>: PR review (AI + SonarQube/Semgrep), PHP migration (Rector + PHPStan), test scaffolding (AI-assisted PHPUnit). Documentation and runbooks can follow.</li><li><strong>Tool mix</strong>: Rector and PHPStan as core; one SaaS AI PR review tool; AI in IDE for tests and docs as <em>drafts</em>.</li><li><strong>Governance</strong>: One-page AI policy; “no training on our data” where possible; audit trail for AI-assisted PRs.</li><li><strong>Pilot</strong>: 90-day plan with baseline metrics, one Rector migration, one test-scaffolding experiment, and a final report with KPIs and risks.</li><li><strong>Risks</strong>: Mitigate over-reliance and hallucination with tests, static analysis, and human review; align with client and contract on data use.</li></ol><p><strong>Immediate next steps</strong></p><ul><li>[ ] Choose two pilot repos (one WP, one Laravel) and get stakeholder sign-off.</li><li>[ ] Draft and approve “AI in our pipeline” policy (data, tools, review rules).</li><li>[ ] Enable PHPStan + Rector in CI and add baseline; enable one AI PR review tool.</li><li>[ ] Record baseline KPIs (PR cycle time, time to first review, coverage sample).</li><li>[ ] Schedule Week 4 and Week 8 pilot reviews and the 90-day report.</li></ul><hr><p><em>Document generated for structural integration of AI into WordPress + Laravel development pipelines. Update as tools and organisational context change.</em></p>',108)])])}const p=e(n,[["render",i]]);export{h as __pageData,p as default};
