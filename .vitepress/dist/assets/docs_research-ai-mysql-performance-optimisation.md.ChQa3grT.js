import{_ as t,o as a,c as o,a0 as r}from"./chunks/framework.B3sz4m_N.js";const u=JSON.parse('{"title":"Research: AI-Assisted MySQL/MariaDB Performance Optimisation for WordPress and Laravel","description":"","frontmatter":{},"headers":[],"relativePath":"docs/research-ai-mysql-performance-optimisation.md","filePath":"docs/research-ai-mysql-performance-optimisation.md"}'),n={name:"docs/research-ai-mysql-performance-optimisation.md"};function i(s,e,l,d,c,g){return a(),o("div",null,[...e[0]||(e[0]=[r('<h1 id="research-ai-assisted-mysql-mariadb-performance-optimisation-for-wordpress-and-laravel" tabindex="-1">Research: AI-Assisted MySQL/MariaDB Performance Optimisation for WordPress and Laravel <a class="header-anchor" href="#research-ai-assisted-mysql-mariadb-performance-optimisation-for-wordpress-and-laravel" aria-label="Permalink to &quot;Research: AI-Assisted MySQL/MariaDB Performance Optimisation for WordPress and Laravel&quot;">​</a></h1><p><strong>Focus areas:</strong> Slow query log analysis, index recommendations, query refactoring, cache layer tuning, partitioning strategies.</p><p><strong>Deliverables:</strong> (1) Techniques for feeding DB schema into LLMs, (2) Risk analysis, (3) Tool comparison, (4) Safe experimentation framework, (5) Performance measurement approach.</p><hr><h2 id="_1-techniques-for-feeding-db-schema-into-llms" tabindex="-1">1. Techniques for Feeding DB Schema into LLMs <a class="header-anchor" href="#_1-techniques-for-feeding-db-schema-into-llms" aria-label="Permalink to &quot;1. Techniques for Feeding DB Schema into LLMs&quot;">​</a></h2><h3 id="_1-1-what-to-extract-and-how" tabindex="-1">1.1 What to Extract and How <a class="header-anchor" href="#_1-1-what-to-extract-and-how" aria-label="Permalink to &quot;1.1 What to Extract and How&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Source</th><th>What to extract</th><th>How (MySQL/MariaDB)</th></tr></thead><tbody><tr><td><strong>Schema</strong></td><td>Tables, columns, types, keys, FKs</td><td><code>INFORMATION_SCHEMA.COLUMNS</code>, <code>TABLES</code>, <code>KEY_COLUMN_USAGE</code>, <code>STATISTICS</code></td></tr><tr><td><strong>Execution context</strong></td><td>Plan, cost, row estimates</td><td><code>EXPLAIN FORMAT=JSON</code> or <code>EXPLAIN ANALYZE</code> (MySQL 8.0+)</td></tr><tr><td><strong>Slow queries</strong></td><td>SQL, duration, rows examined</td><td>Slow query log, Query Monitor (WP), Telescope (Laravel), <code>performance_schema</code></td></tr><tr><td><strong>Indexes</strong></td><td>Existing indexes, cardinality</td><td><code>SHOW INDEX FROM table</code> or <code>INFORMATION_SCHEMA.STATISTICS</code></td></tr></tbody></table><h3 id="_1-2-recommended-context-formats" tabindex="-1">1.2 Recommended Context Formats <a class="header-anchor" href="#_1-2-recommended-context-formats" aria-label="Permalink to &quot;1.2 Recommended Context Formats&quot;">​</a></h3><ul><li><strong>Schema → LLM:</strong> DDL or a compact text summary: table name, column list, primary/unique keys, foreign keys, and (if needed) sample row counts. Avoid dumping full <code>CREATE TABLE</code> with engine/options unless tuning storage.</li><li><strong>Execution plans → LLM:</strong> Prefer <strong>EXPLAIN FORMAT=JSON</strong> (and <strong>EXPLAIN ANALYZE</strong> where available). JSON is machine-readable and gives cost, access type, key usage, and row estimates in a structure LLMs handle well.</li><li><strong>Slow query payload:</strong> For each slow query, provide: (1) SQL (normalised/redacted if needed), (2) duration and rows examined, (3) EXPLAIN JSON, (4) table names involved so the model can tie back to schema.</li></ul><h3 id="_1-3-implementation-options" tabindex="-1">1.3 Implementation Options <a class="header-anchor" href="#_1-3-implementation-options" aria-label="Permalink to &quot;1.3 Implementation Options&quot;">​</a></h3><ul><li><strong>Manual pipeline:</strong> Script that queries <code>INFORMATION_SCHEMA</code> + slow log/export, runs <code>EXPLAIN</code> for each query, and builds a single markdown or JSON “pack” to paste into an LLM.</li><li><strong>Vanna (vanna.ai):</strong> Train on your MySQL schema (and optionally sample queries); use for Text-to-SQL and schema-aware Q&amp;A. Supports MySQL and can feed schema + questions into an LLM.</li><li><strong>LlamaIndex:</strong> Use SQL/structured-data workflows to load schema and query results, then send structured context to an LLM for analysis or index suggestions.</li><li><strong>Custom RAG:</strong> Index schema metadata (and optionally EXPLAIN outputs) in a vector store; retrieve relevant tables/plans when asking “suggest indexes for this query.”</li></ul><h3 id="_1-4-scope-control" tabindex="-1">1.4 Scope Control <a class="header-anchor" href="#_1-4-scope-control" aria-label="Permalink to &quot;1.4 Scope Control&quot;">​</a></h3><ul><li><strong>WordPress:</strong> Restrict schema export to <code>wp_*</code> tables (and known plugin tables) to keep context small and relevant.</li><li><strong>Laravel:</strong> Export only tables used by the app (e.g. from migrations or a DB list). Include <code>schema_migrations</code> for context if useful.</li><li><strong>Token budget:</strong> Summarise large schemas (e.g. “table X: columns A,B,C; PK on A; index on B”) and send full DDL only for tables referenced in the slow queries you’re optimising.</li></ul><hr><h2 id="_2-risk-analysis-hallucinated-indexes-unsafe-changes" tabindex="-1">2. Risk Analysis: Hallucinated Indexes, Unsafe Changes <a class="header-anchor" href="#_2-risk-analysis-hallucinated-indexes-unsafe-changes" aria-label="Permalink to &quot;2. Risk Analysis: Hallucinated Indexes, Unsafe Changes&quot;">​</a></h2><h3 id="_2-1-hallucination-and-incorrectness" tabindex="-1">2.1 Hallucination and Incorrectness <a class="header-anchor" href="#_2-1-hallucination-and-incorrectness" aria-label="Permalink to &quot;2.1 Hallucination and Incorrectness&quot;">​</a></h3><ul><li><strong>Plausible but wrong indexes:</strong> LLMs can suggest indexes that don’t match the actual query (wrong column order, wrong table, or redundant with existing indexes).</li><li><strong>Invalid or unsupported syntax:</strong> Generated <code>CREATE INDEX</code> or <code>ALTER TABLE</code> might use dialect-specific or deprecated syntax (e.g. fulltext vs B-tree, index hints).</li><li><strong>Over-indexing:</strong> Suggesting many indexes that help a few queries but hurt write throughput and storage.</li><li><strong>Wrong partitioning strategy:</strong> Suggesting partitioning when the main gain would come from indexing or query rewrite, or choosing poor partition keys.</li></ul><h3 id="_2-2-safety-risks" tabindex="-1">2.2 Safety Risks <a class="header-anchor" href="#_2-2-safety-risks" aria-label="Permalink to &quot;2.2 Safety Risks&quot;">​</a></h3><ul><li><strong>Destructive or blocking changes:</strong> <code>ALTER TABLE</code> that locks tables, drops indexes still in use, or changes column types and breaks application code.</li><li><strong>No rollback plan:</strong> Applying changes without a way to revert (backup, invisible index, or staged rollout).</li><li><strong>Cache/config recommendations:</strong> Suggesting changes to <code>innodb_buffer_pool_size</code> or other globals without considering total RAM and other processes.</li></ul><h3 id="_2-3-mitigation-strategies" tabindex="-1">2.3 Mitigation Strategies <a class="header-anchor" href="#_2-3-mitigation-strategies" aria-label="Permalink to &quot;2.3 Mitigation Strategies&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Risk</th><th>Mitigation</th></tr></thead><tbody><tr><td>Hallucinated indexes</td><td>Never apply index suggestions without validating: run <code>EXPLAIN</code> before/after (or on a copy), compare row/cost estimates; prefer tools that use DB feedback (e.g. HeatWave Autopilot).</td></tr><tr><td>Invalid SQL</td><td>Validate every generated DDL (syntax check, dry-run where supported, apply on a clone first).</td></tr><tr><td>Over-indexing</td><td>Use “Index-Guided Major Voting” or similar: only adopt suggestions that appear in multiple analyses or that show clear EXPLAIN improvement.</td></tr><tr><td>Destructive changes</td><td>Use branching/workflow (e.g. PlanetScale-style), staging DB, or pt-online-schema-change with pre-checks; make indexes invisible before DROP.</td></tr><tr><td>Config tuning</td><td>Treat as advisory only; test on staging with realistic load; monitor buffer pool hit ratio and other metrics.</td></tr></tbody></table><h3 id="_2-4-principle" tabindex="-1">2.4 Principle <a class="header-anchor" href="#_2-4-principle" aria-label="Permalink to &quot;2.4 Principle&quot;">​</a></h3><p><strong>Treat every AI suggestion as a hypothesis.</strong> Validate with real execution plans, staging, and measurable performance criteria before production.</p><hr><h2 id="_3-tool-comparison" tabindex="-1">3. Tool Comparison <a class="header-anchor" href="#_3-tool-comparison" aria-label="Permalink to &quot;3. Tool Comparison&quot;">​</a></h2><h3 id="_3-1-ai-oriented-llm-based" tabindex="-1">3.1 AI-Oriented / LLM-Based <a class="header-anchor" href="#_3-1-ai-oriented-llm-based" aria-label="Permalink to &quot;3.1 AI-Oriented / LLM-Based&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Tool / Approach</th><th>Scope</th><th>Strengths</th><th>Limitations</th></tr></thead><tbody><tr><td><strong>Google Cloud SQL (Gemini Cloud Assist)</strong></td><td>Slow query troubleshooting</td><td>Built-in anomaly detection, recommendations, 24h baseline</td><td>Cloud SQL only; enterprise config limits; preview state</td></tr><tr><td><strong>MariaDB AI (Copilot / MCP)</strong></td><td>Tuning, debugging, NL queries</td><td>Native integration, DBA/Dev Copilot, MCP for AI tooling</td><td>MariaDB ecosystem</td></tr><tr><td><strong>Vanna.AI</strong></td><td>Text-to-SQL, schema Q&amp;A</td><td>Trains on your schema, MySQL support, vector-backed context</td><td>General SQL generation; index advice requires explicit prompting and validation</td></tr><tr><td><strong>LlamaIndex (+ MySQL)</strong></td><td>Custom RAG / workflows</td><td>Flexible schema + query context, multiple LLM backends</td><td>You build the pipeline and safeguards</td></tr><tr><td><strong>Aiven AI Database Optimiser</strong></td><td>Always-on tuning</td><td>Continuous monitoring, index/slow-query recommendations, free tier for Postgres/MySQL</td><td>SaaS; dependency on Aiven</td></tr><tr><td><strong>D-Bot / RCRank (research)</strong></td><td>Root-cause ranking, diagnosis</td><td>Multimodal (query + plan + logs), prioritises causes</td><td>Academic; not off-the-shelf products</td></tr></tbody></table><h3 id="_3-2-traditional-non-llm-validation-and-baseline" tabindex="-1">3.2 Traditional / Non-LLM (Validation and Baseline) <a class="header-anchor" href="#_3-2-traditional-non-llm-validation-and-baseline" aria-label="Permalink to &quot;3.2 Traditional / Non-LLM (Validation and Baseline)&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Tool</th><th>Use case</th><th>Notes</th></tr></thead><tbody><tr><td><strong>MySQL HeatWave Autopilot Indexing</strong></td><td>Index recommendations with impact analysis</td><td>Review UI, estimated speedup/storage; apply with confirmation; invisible index for safe DROP.</td></tr><tr><td><strong>pt-query-digest / Percona Toolkit</strong></td><td>Slow query log analysis</td><td>Aggregate and rank slow queries; feed output + EXPLAIN to LLM.</td></tr><tr><td><strong>MySQL PERFORMANCE_SCHEMA / Sys schema</strong></td><td>Live metrics, wait events</td><td>Data for tuning and for validating AI-suggested changes.</td></tr><tr><td><strong>Query Monitor (WordPress)</strong></td><td>Per-request queries, callers</td><td>Export/copy slow queries and stack traces for LLM context.</td></tr><tr><td><strong>Laravel Telescope + Enlightn</strong></td><td>Slow and N+1 query detection</td><td>Identify queries and code paths; export for analysis.</td></tr></tbody></table><h3 id="_3-3-fit-by-stack" tabindex="-1">3.3 Fit by Stack <a class="header-anchor" href="#_3-3-fit-by-stack" aria-label="Permalink to &quot;3.3 Fit by Stack&quot;">​</a></h3><ul><li><strong>WordPress:</strong> Query Monitor + slow query log (or VIP slow logs) → extract schema (e.g. <code>wp_*</code>) + EXPLAIN → LLM or Vanna. Use traditional DB tools to validate.</li><li><strong>Laravel:</strong> Telescope/Enlightn + slow query log → schema from migrations or DB → same pipeline. Consider Aiven or MariaDB AI if already on those platforms.</li></ul><hr><h2 id="_4-safe-experimentation-framework" tabindex="-1">4. Safe Experimentation Framework <a class="header-anchor" href="#_4-safe-experimentation-framework" aria-label="Permalink to &quot;4. Safe Experimentation Framework&quot;">​</a></h2><h3 id="_4-1-principles" tabindex="-1">4.1 Principles <a class="header-anchor" href="#_4-1-principles" aria-label="Permalink to &quot;4.1 Principles&quot;">​</a></h3><ul><li><strong>No direct production changes from AI output.</strong> All schema/index changes go through a pipeline that includes validation and staging.</li><li><strong>Backward compatibility:</strong> Prefer additive changes (new indexes, new columns with defaults). For drops, use invisible index first, then drop after validation.</li><li><strong>Isolated testing:</strong> Use a clone or staging DB with realistic data volume and load (e.g. anonymised copy, or production-like dataset).</li></ul><h3 id="_4-2-pipeline-high-level" tabindex="-1">4.2 Pipeline (High Level) <a class="header-anchor" href="#_4-2-pipeline-high-level" aria-label="Permalink to &quot;4.2 Pipeline (High Level)&quot;">​</a></h3><ol><li><strong>Capture:</strong> Slow query log + schema (and optionally EXPLAIN for top N queries).</li><li><strong>Generate:</strong> LLM or AI tool suggests indexes, query rewrites, or config changes.</li><li><strong>Validate:</strong><ul><li>Syntax-check DDL; run on a <strong>staging/clone</strong> only.</li><li>Run EXPLAIN (and EXPLAIN ANALYZE if available) before/after for affected queries.</li><li>Run a small benchmark (e.g. mysqlslap, SysBench, or app-level tests) and compare metrics.</li></ul></li><li><strong>Stage:</strong> Apply to staging; run integration and performance tests; monitor for regressions.</li><li><strong>Release:</strong> Apply to production via safe migration (e.g. pt-osc, gh-ost, or managed DB deploy requests) with rollback plan.</li><li><strong>Observe:</strong> Monitor slow query log, latency, and throughput; revert (e.g. make index visible again or drop new index) if metrics regress.</li></ol><h3 id="_4-3-environment-progression" tabindex="-1">4.3 Environment Progression <a class="header-anchor" href="#_4-3-environment-progression" aria-label="Permalink to &quot;4.3 Environment Progression&quot;">​</a></h3><ul><li><strong>Local / dev</strong> → <strong>Staging / UAT</strong> → <strong>Production.</strong></li><li>Use schema fingerprinting or migration checks to keep staging and production in sync before applying changes.</li><li>For high-risk changes, use a <strong>canary</strong> (e.g. one app instance or read replica) before full rollout.</li></ul><h3 id="_4-4-rollback" tabindex="-1">4.4 Rollback <a class="header-anchor" href="#_4-4-rollback" aria-label="Permalink to &quot;4.4 Rollback&quot;">​</a></h3><ul><li><strong>New indexes:</strong> Drop index if write load or storage is worse and no query benefit is seen.</li><li><strong>Dropped indexes:</strong> Use “invisible index” so drop is reversible; after a period, make visible again if needed, else complete the drop.</li><li><strong>Config:</strong> Revert to previous values; document and version config changes.</li></ul><hr><h2 id="_5-performance-measurement-approach" tabindex="-1">5. Performance Measurement Approach <a class="header-anchor" href="#_5-performance-measurement-approach" aria-label="Permalink to &quot;5. Performance Measurement Approach&quot;">​</a></h2><h3 id="_5-1-what-to-measure" tabindex="-1">5.1 What to Measure <a class="header-anchor" href="#_5-1-what-to-measure" aria-label="Permalink to &quot;5.1 What to Measure&quot;">​</a></h3><ul><li><strong>Query-level:</strong> Duration, rows examined, rows returned, lock time (from slow log or <code>performance_schema</code>).</li><li><strong>System-level:</strong> CPU, IO, QPS, buffer pool hit ratio (InnoDB), connection count.</li><li><strong>Application-level:</strong> End-to-end response time, time-to-first-byte, error rate (e.g. Server-Timing in WordPress, APM in Laravel).</li></ul><h3 id="_5-2-baseline-and-comparison" tabindex="-1">5.2 Baseline and Comparison <a class="header-anchor" href="#_5-2-baseline-and-comparison" aria-label="Permalink to &quot;5.2 Baseline and Comparison&quot;">​</a></h3><ul><li><strong>Baseline:</strong> Capture metrics (e.g. P95/P99 query time, QPS, buffer hit ratio) before any change. Use the same workload (recorded or synthetic).</li><li><strong>Consistency:</strong> Same hardware/OS, same dataset size, same config except the change under test. Run multiple iterations (e.g. 20–100 requests per scenario) to account for variance.</li><li><strong>Locally:</strong> Prefer local or staging benchmarks to avoid production noise; use production-like data and config where possible.</li></ul><h3 id="_5-3-tools-and-practices" tabindex="-1">5.3 Tools and Practices <a class="header-anchor" href="#_5-3-tools-and-practices" aria-label="Permalink to &quot;5.3 Tools and Practices&quot;">​</a></h3><ul><li><strong>MySQL:</strong> Slow query log with threshold (e.g. 1s); <code>EXPLAIN ANALYZE</code>; <code>performance_schema</code> and <code>sys</code> schema; mysqlslap, SysBench, or DBT2 for load.</li><li><strong>WordPress:</strong> Query Monitor for per-request analysis; Server-Timing and performance handbook practices; WP-CLI or custom scripts for repeated runs.</li><li><strong>Laravel:</strong> Telescope for slow queries; Enlightn analyzers; replay or scripted requests for benchmarks.</li><li><strong>Extended tests:</strong> For tuning (e.g. buffer pool, cache), run sustained load (e.g. Locust) for hours or days to capture cache and IO behaviour.</li></ul><h3 id="_5-4-success-criteria-for-an-ai-suggested-change" tabindex="-1">5.4 Success Criteria for an AI-Suggested Change <a class="header-anchor" href="#_5-4-success-criteria-for-an-ai-suggested-change" aria-label="Permalink to &quot;5.4 Success Criteria for an AI-Suggested Change&quot;">​</a></h3><ul><li><strong>Index / query change:</strong> Measurable improvement in latency or rows examined for the targeted query(s), without material regression in other queries or write throughput.</li><li><strong>Cache/config:</strong> Improved buffer hit ratio or throughput under load, with no OOM or instability.</li><li><strong>Partitioning:</strong> Improved query latency and/or maintenance time (e.g. backup, purge), validated on a copy with representative data size.</li></ul><hr><h2 id="summary" tabindex="-1">Summary <a class="header-anchor" href="#summary" aria-label="Permalink to &quot;Summary&quot;">​</a></h2><ul><li><strong>Feeding schema into LLMs:</strong> Use <code>INFORMATION_SCHEMA</code> (and optionally DDL), EXPLAIN in JSON, and slow query excerpts; keep scope small (relevant tables only) and use Vanna/LlamaIndex or a custom script for context.</li><li><strong>Risks:</strong> Hallucinated or suboptimal indexes, invalid DDL, over-indexing, destructive changes. Mitigate by validating with EXPLAIN, staging, invisible index for drops, and never applying AI output blindly.</li><li><strong>Tools:</strong> Mix of cloud/AI (Google Cloud Assist, MariaDB AI, Vanna, Aiven) and traditional (pt-query-digest, HeatWave Autopilot, Query Monitor, Telescope); choose by stack and where your data lives.</li><li><strong>Safe experimentation:</strong> Clone/staging → validate DDL and EXPLAIN → benchmark → staged rollout with rollback (invisible index, config revert).</li><li><strong>Measurement:</strong> Define baselines, use consistent workload and multiple runs, and tie success to query and system metrics (latency, rows examined, buffer hit ratio, QPS) before and after each change.</li></ul><p>Using this structure, you can integrate AI-assisted analysis into your MySQL/MariaDB workflow for WordPress and Laravel while keeping changes safe, measurable, and reversible.</p>',55)])])}const m=t(n,[["render",i]]);export{u as __pageData,m as default};
