import{_ as e,o as a,c as n,a0 as s}from"./chunks/framework.B3sz4m_N.js";const h=JSON.parse('{"title":"AI Transformation Whitepaper","description":"Full strategic programme — workstreams, prioritisation, roadmap, PoCs, governance, maturity.","frontmatter":{"title":"AI Transformation Whitepaper","description":"Full strategic programme — workstreams, prioritisation, roadmap, PoCs, governance, maturity."},"headers":[],"relativePath":"reports/ai-transformation-programme/01-whitepaper.md","filePath":"reports/ai-transformation-programme/01-whitepaper.md"}'),o={name:"reports/ai-transformation-programme/01-whitepaper.md"};function r(i,t,d,l,p,c){return a(),n("div",null,[...t[0]||(t[0]=[s(`<h1 id="ai-transformation-research-programme-—-whitepaper" tabindex="-1">AI Transformation Research Programme — Whitepaper <a class="header-anchor" href="#ai-transformation-research-programme-—-whitepaper" aria-label="Permalink to &quot;AI Transformation Research Programme — Whitepaper&quot;">​</a></h1><p><strong>Audience:</strong> Engineering leads, delivery managers, DevOps, security/compliance.<br><strong>Context:</strong> WordPress (ACF-heavy, Classic Editor) + Laravel (queues, APIs, Passport); client retainer work; PHP 7.4→8.x; Redis, MySQL, caching; UK/GDPR.<br><strong>Objectives:</strong> Engineering efficiency, reduced legacy burden, observability, code quality and test coverage, client-facing AI value where appropriate, governance and data safety.</p><p>This document defines research workstreams, prioritisation, a 12-month roadmap, proof-of-concept templates, governance and policy, and a capability maturity model. It aligns with the existing <a href="./reports/ai-governance/01-governance-framework-draft.html">AI Governance Framework</a> and <a href="./reports/ai-governance/03-tooling-stack.html">Tooling Stack</a>.</p><hr><h2 id="step-1-research-workstreams" tabindex="-1">STEP 1: Research Workstreams <a class="header-anchor" href="#step-1-research-workstreams" aria-label="Permalink to &quot;STEP 1: Research Workstreams&quot;">​</a></h2><p>Six major AI research pillars. Each addresses specific problems, strategic importance, disruption level, ROI horizon, and risks.</p><table tabindex="0"><thead><tr><th>Pillar</th><th>Problem it solves</th><th>Strategic importance</th><th>Disruption</th><th>ROI horizon</th><th>Risks</th></tr></thead><tbody><tr><td><strong>1. AI-assisted development &amp; PR review</strong></td><td>Review bottleneck; inconsistent security/sanitisation checks; slow first-pass feedback.</td><td>High — directly reduces cycle time and improves consistency across WP/Laravel.</td><td>Low</td><td>Short</td><td>Over-reliance on AI review; false confidence; code/data sent to SaaS (see governance).</td></tr><tr><td><strong>2. Legacy modernisation &amp; migration</strong></td><td>PHP 7.4→8.x and WP/ACF upgrade burden; manual, error-prone refactors.</td><td>High — unblocks security and performance; reduces long-term support cost.</td><td>Medium</td><td>Medium</td><td>Regression; incomplete migrations; baseline creep if not managed.</td></tr><tr><td><strong>3. Test generation &amp; coverage</strong></td><td>Low coverage on legacy codebases; tedious scaffolding; regression gaps.</td><td>High — improves confidence for refactors and client changes.</td><td>Low</td><td>Short</td><td>Brittle or irrelevant tests; coverage metrics gamed; no ownership of AI-generated tests.</td></tr><tr><td><strong>4. Observability &amp; incident response</strong></td><td>Reactive firefighting; poor visibility into queues, APIs, and WP performance.</td><td>High — faster MTTR; better client communication; data for prioritisation.</td><td>Medium</td><td>Medium</td><td>Alert fatigue; cost of tooling; PII in logs if not controlled.</td></tr><tr><td><strong>5. Documentation &amp; knowledge capture</strong></td><td>Runbooks, ADRs, API docs out of date; onboarding slow; handover risk.</td><td>Medium — reduces bus factor and retainer handover cost.</td><td>Low</td><td>Medium</td><td>Stale or wrong AI-generated docs if not owned and reviewed.</td></tr><tr><td><strong>6. Client-facing AI &amp; value</strong></td><td>Client demand for “AI features”; differentiation; upsell.</td><td>Medium — revenue and retention; must be scoped and governed.</td><td>High</td><td>Long</td><td>Scope creep; PII/consent; liability; over-promising.</td></tr><tr><td><strong>7. Governance, DLP &amp; safe deployment</strong></td><td>Data leakage; unapproved tools; client and GDPR exposure.</td><td>Critical — enabler for all other pillars; non-negotiable for UK clients.</td><td>Low (if done early)</td><td>Short</td><td>Policy bypass (shadow AI); slow approval if process is heavy.</td></tr><tr><td><strong>8. Agentic / semi-autonomous tooling</strong></td><td>Repetitive tasks (releases, dependency bumps, boilerplate); scaling without linear headcount.</td><td>Medium–High — efficiency multiplier; experimental.</td><td>High</td><td>Long</td><td>Uncontrolled changes; security and quality regressions; accountability gaps.</td></tr></tbody></table><hr><h3 id="workstream-summaries" tabindex="-1">Workstream summaries <a class="header-anchor" href="#workstream-summaries" aria-label="Permalink to &quot;Workstream summaries&quot;">​</a></h3><p><strong>1. AI-assisted development &amp; PR review</strong><br> Standardise on an approved AI PR review step (e.g. CodeRabbit, Graphite Agent, or Semgrep + AI) in CI; define a short PR review policy (sanitisation, nonces, capabilities, validation) and feed into tool context. Measure: time to first review, issues found in review vs post-merge.</p><p><strong>2. Legacy modernisation &amp; migration</strong><br> Rector as backbone for PHP and framework upgrades; PHPStan/Psalm with baselines for legacy. AI (Cursor/Copilot) for <em>suggestions</em> and drafting only. Scope by file or feature (single ACF block, single API module).</p><p><strong>3. Test generation &amp; coverage</strong><br> AI for PHPUnit/Codeception scaffolding and “test that would have caught this bug”; human writes/edits assertions and owns tests. Tie to Definition of Done (e.g. test suggestion on bug closure).</p><p><strong>4. Observability &amp; incident response</strong><br> Structured logging, correlation IDs, queue/API metrics; optional AI-assisted log summarisation and runbook suggestion (with redaction). Server-Timing, WP-CLI profile, Query Monitor patterns for WP.</p><p><strong>5. Documentation &amp; knowledge capture</strong><br> PHPDoc and API docs from code (OpenAPI from routes); AI to draft runbooks/ADRs from history; clear ownership and review before docs become canonical.</p><p><strong>6. Client-facing AI</strong><br> Scoped use cases only (e.g. internal search, content suggestions) with explicit client approval, legal basis, and no PII to third-party models unless contracted. Tier 3 in governance.</p><p><strong>7. Governance, DLP &amp; safe deployment</strong><br> Implement risk tiers (1/2/3), approved tool list, PII redaction before SaaS AI, and safe deployment checklist. See <a href="./reports/ai-governance/01-governance-framework-draft.html">Governance Framework</a> and <a href="./reports/ai-governance/02-risk-register-template.html">Risk Register</a>.</p><p><strong>8. Agentic / semi-autonomous</strong><br> Pilots only: e.g. automated dependency bumps with PR + CI, or release packaging with human gate. Clear boundaries and rollback.</p><hr><h2 id="step-2-prioritisation-matrix" tabindex="-1">STEP 2: Prioritisation Matrix <a class="header-anchor" href="#step-2-prioritisation-matrix" aria-label="Permalink to &quot;STEP 2: Prioritisation Matrix&quot;">​</a></h2><p>Ranking by impact, implementation difficulty, risk, resource requirement, and time to measurable value. Scores are indicative (1–5, 5 = highest impact / hardest / riskiest / most resource / slowest value).</p><table tabindex="0"><thead><tr><th>Workstream</th><th>Impact</th><th>Impl. difficulty</th><th>Risk</th><th>Resource</th><th>Time to value</th><th>Notes</th></tr></thead><tbody><tr><td>Governance, DLP &amp; safe deployment</td><td>5</td><td>2</td><td>2</td><td>2</td><td>1 (fast)</td><td>Foundation; unblocks other work.</td></tr><tr><td>AI-assisted dev &amp; PR review</td><td>5</td><td>2</td><td>3</td><td>2</td><td>1</td><td>Well-understood tools; policy doc required.</td></tr><tr><td>Test generation &amp; coverage</td><td>5</td><td>2</td><td>2</td><td>2</td><td>1</td><td>Scaffolding + human ownership.</td></tr><tr><td>Legacy modernisation &amp; migration</td><td>5</td><td>4</td><td>3</td><td>4</td><td>2</td><td>Rector + PHPStan proven; scope and baselines matter.</td></tr><tr><td>Observability &amp; incident response</td><td>4</td><td>3</td><td>2</td><td>3</td><td>2</td><td>Instrumentation first; AI summarisation optional.</td></tr><tr><td>Documentation &amp; knowledge capture</td><td>3</td><td>2</td><td>2</td><td>2</td><td>2</td><td>Low risk; clear ownership.</td></tr><tr><td>Client-facing AI</td><td>4</td><td>4</td><td>5</td><td>4</td><td>3</td><td>High risk and scope; Tier 3 approval.</td></tr><tr><td>Agentic / semi-autonomous</td><td>4</td><td>5</td><td>4</td><td>3</td><td>3</td><td>Experimental; bounded pilots only.</td></tr></tbody></table><hr><h3 id="top-3-quick-wins" tabindex="-1">Top 3 Quick Wins <a class="header-anchor" href="#top-3-quick-wins" aria-label="Permalink to &quot;Top 3 Quick Wins&quot;">​</a></h3><table tabindex="0"><thead><tr><th>#</th><th>Initiative</th><th>Rationale</th></tr></thead><tbody><tr><td>1</td><td><strong>Governance baseline</strong></td><td>Publish internal AI usage policy, Tier 1/2/3 model, approved tool list, and “no PII in prompts” rule. Enables safe adoption of all other initiatives.</td></tr><tr><td>2</td><td><strong>AI PR review pilot</strong></td><td>Add one AI review tool (e.g. CodeRabbit or Graphite Agent) to a single repo with a 4-week trial; define PR review policy; measure time to first review and catch rate.</td></tr><tr><td>3</td><td><strong>Test scaffolding from AI</strong></td><td>Standardise “suggest PHPUnit test for this fix” in DoD; use Cursor/Copilot/PhpStorm for scaffolds only; track new tests per sprint and failure catch rate.</td></tr></tbody></table><hr><h3 id="top-3-strategic-investments" tabindex="-1">Top 3 Strategic Investments <a class="header-anchor" href="#top-3-strategic-investments" aria-label="Permalink to &quot;Top 3 Strategic Investments&quot;">​</a></h3><table tabindex="0"><thead><tr><th>#</th><th>Initiative</th><th>Rationale</th></tr></thead><tbody><tr><td>1</td><td><strong>Legacy modernisation pipeline</strong></td><td>Rector (PHP 8.x rules) + PHPStan (baseline) + test suite on a copy of one client codebase; document migration runbooks; reduces long-term support and unblocks PHP 8.x.</td></tr><tr><td>2</td><td><strong>Observability foundation</strong></td><td>Structured logs, correlation IDs, queue/API metrics, and optional AI-assisted summarisation (redacted); improves MTTR and client communication.</td></tr><tr><td>3</td><td><strong>Client-facing AI playbook</strong></td><td>Define scoped use cases, approval flow (Tier 3), legal basis, and tooling (e.g. on-prem or redacted SaaS); one pilot client before broader rollout.</td></tr></tbody></table><hr><h2 id="step-3-phased-roadmap-12-months" tabindex="-1">STEP 3: Phased Roadmap (12 Months) <a class="header-anchor" href="#step-3-phased-roadmap-12-months" aria-label="Permalink to &quot;STEP 3: Phased Roadmap (12 Months)&quot;">​</a></h2><h3 id="phase-1-—-0–3-months-low-risk-pilots" tabindex="-1">Phase 1 — 0–3 months: Low-risk pilots <a class="header-anchor" href="#phase-1-—-0–3-months-low-risk-pilots" aria-label="Permalink to &quot;Phase 1 — 0–3 months: Low-risk pilots&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Deliverable</th><th>Tooling</th><th>Ownership</th><th>Success metrics</th></tr></thead><tbody><tr><td>Internal AI usage policy and Tier 1/2/3 approval model</td><td>Policy doc; optional intake form (Forms/Notion)</td><td>Security/Compliance + Delivery</td><td>Policy published; no Tier 3 use without approval.</td></tr><tr><td>Approved AI tools list (IDE, PR review, SAST)</td><td>Governance doc; <a href="./reports/ai-governance/03-tooling-stack.html">Tooling Stack</a></td><td>Security/Compliance</td><td>List agreed; devs trained on “approved only”.</td></tr><tr><td>AI PR review on one repo (4–6 weeks)</td><td>CodeRabbit, Graphite Agent, or Semgrep + config</td><td>Backend / WP (repo owner)</td><td>Time to first review ↓; issues found in review vs post-merge.</td></tr><tr><td>Test scaffolding workflow</td><td>Cursor/Copilot/PhpStorm; PHPUnit/Codeception</td><td>Backend / WP</td><td>New tests per sprint; tests tied to bug fixes.</td></tr><tr><td>PII redaction / “no raw PII to SaaS” enforcement</td><td>Policy + training; optional gateway later</td><td>Security/Compliance</td><td>No incidents; spot checks on prompts.</td></tr></tbody></table><p><strong>Phase 1 tooling:</strong> Existing CI (e.g. GitHub Actions); one AI PR review tool (see <a href="./reports/ai-wp-laravel-pipeline/01-research-document.html">Pipeline research</a>); IDE AI (Cursor/Copilot) under policy.</p><hr><h3 id="phase-2-—-3–6-months-structural-improvements" tabindex="-1">Phase 2 — 3–6 months: Structural improvements <a class="header-anchor" href="#phase-2-—-3–6-months-structural-improvements" aria-label="Permalink to &quot;Phase 2 — 3–6 months: Structural improvements&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Deliverable</th><th>Tooling</th><th>Ownership</th><th>Success metrics</th></tr></thead><tbody><tr><td>Rector + PHPStan migration pipeline (one codebase)</td><td>Rector, PHPStan, Composer; baseline</td><td>Backend / WP</td><td>Files upgraded; test pass rate; rollback count.</td></tr><tr><td>Observability: structured logging + correlation IDs</td><td>Laravel logging; WP logging plugin or custom</td><td>DevOps + Backend</td><td>Logs queryable; MTTR baseline.</td></tr><tr><td>Queue and API metrics (throughput, failure rate)</td><td>Laravel Horizon / Telescope; Prometheus/Grafana or vendor</td><td>DevOps</td><td>Dashboards live; alerts defined.</td></tr><tr><td>Documentation pass: PHPDoc and API docs from code</td><td>IDE or CI job; OpenAPI from routes</td><td>Backend / WP</td><td>Coverage of changed code; API docs up to date.</td></tr><tr><td>Client approval register (Tier 3) and re-approval cadence</td><td>Register (Confluence/SharePoint); annual re-confirm</td><td>Delivery + Compliance</td><td>All Tier 3 uses documented; no unapproved PII.</td></tr></tbody></table><p><strong>Phase 2 tooling:</strong> Rector, PHPStan, Laravel Horizon/Telescope, existing APM or Prometheus; OpenAPI tooling (e.g. L5-Swagger, or manual from routes).</p><hr><h3 id="phase-3-—-6–12-months-advanced-agentic" tabindex="-1">Phase 3 — 6–12 months: Advanced / agentic <a class="header-anchor" href="#phase-3-—-6–12-months-advanced-agentic" aria-label="Permalink to &quot;Phase 3 — 6–12 months: Advanced / agentic&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Deliverable</th><th>Tooling</th><th>Ownership</th><th>Success metrics</th></tr></thead><tbody><tr><td>AI-assisted log/incident summarisation (redacted)</td><td>Log aggregation + LLM API behind gateway; redaction</td><td>DevOps + Security</td><td>Faster runbook suggestion; no PII in prompts.</td></tr><tr><td>Client-facing AI pilot (one use case, one client)</td><td>Scoped feature; on-prem or redacted SaaS</td><td>Cross-team</td><td>Client sign-off; legal basis documented; no leakage.</td></tr><tr><td>Bounded agentic pilot (e.g. dependency bump PRs)</td><td>Scripts + CI; optional agent framework</td><td>Backend / DevOps</td><td>PRs created; human merge rate; no production incidents.</td></tr><tr><td>Maturity assessment and roadmap refresh</td><td>Internal assessment against maturity model (Step 6)</td><td>Lead / PM</td><td>Level achieved; next-phase priorities.</td></tr></tbody></table><p><strong>Phase 3 tooling:</strong> Log aggregation (e.g. Datadog, ELK, or cloud-native); LLM API with gateway/redaction; dependency-update bots (Dependabot, Renovate) or custom.</p><hr><h2 id="step-4-proof-of-concept-templates-top-5-initiatives" tabindex="-1">STEP 4: Proof-of-Concept Templates (Top 5 Initiatives) <a class="header-anchor" href="#step-4-proof-of-concept-templates-top-5-initiatives" aria-label="Permalink to &quot;STEP 4: Proof-of-Concept Templates (Top 5 Initiatives)&quot;">​</a></h2><h3 id="poc-1-ai-pr-review-first-pass" tabindex="-1">PoC 1: AI PR review (first-pass) <a class="header-anchor" href="#poc-1-ai-pr-review-first-pass" aria-label="Permalink to &quot;PoC 1: AI PR review (first-pass)&quot;">​</a></h3><p><strong>Example architecture (text):</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[Developer] --&gt; [Git Push] --&gt; [GitHub]</span></span>
<span class="line"><span>                                |</span></span>
<span class="line"><span>                                v</span></span>
<span class="line"><span>                    [GitHub Actions workflow]</span></span>
<span class="line"><span>                                |</span></span>
<span class="line"><span>                    +-----------+-----------+</span></span>
<span class="line"><span>                    v                       v</span></span>
<span class="line"><span>            [Semgrep / SonarQube]    [AI PR review tool]</span></span>
<span class="line"><span>            (deterministic rules)    (CodeRabbit / Graphite Agent)</span></span>
<span class="line"><span>                    |                       |</span></span>
<span class="line"><span>                    +-----------+-----------+</span></span>
<span class="line"><span>                                v</span></span>
<span class="line"><span>                    [PR comment / status check]</span></span>
<span class="line"><span>                                |</span></span>
<span class="line"><span>                                v</span></span>
<span class="line"><span>                    [Human reviewer] --&gt; [Merge / Request changes]</span></span></code></pre></div><p><strong>Tool stack:</strong> Semgrep or SonarQube (CI); one of CodeRabbit, Graphite Agent, Greptile (trial). Policy doc in repo (e.g. <code>.github/PR_REVIEW_POLICY.md</code>) or in tool context.</p><p><strong>Rough cost:</strong></p><ul><li>Infrastructure: existing CI (no extra).</li><li>API: CodeRabbit/Graphite free tier or ~$10–30/developer/month; Semgrep OSS free, paid for advanced rules.</li></ul><p><strong>Governance:</strong> Tier 2 (client code, no PII). Ensure no client PII in repo/comments; approved tool only; code may leave environment — confirm DPA and no-training clause.</p><p><strong>Failure modes:</strong></p><ul><li>False sense of security (AI misses issues) → keep human review and deterministic SAST.</li><li>Noise → tune policy and tool config; cap comment volume.</li><li>Data sent to vendor → document in register; use only with DPA.</li></ul><hr><h3 id="poc-2-legacy-migration-php-7-4-→-8-x" tabindex="-1">PoC 2: Legacy migration (PHP 7.4 → 8.x) <a class="header-anchor" href="#poc-2-legacy-migration-php-7-4-→-8-x" aria-label="Permalink to &quot;PoC 2: Legacy migration (PHP 7.4 → 8.x)&quot;">​</a></h3><p><strong>Example architecture (text):</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[Client codebase copy] --&gt; [Rector (PHP 8.x rule set)]</span></span>
<span class="line"><span>                                    |</span></span>
<span class="line"><span>                                    v</span></span>
<span class="line"><span>                          [Composer update / autoload]</span></span>
<span class="line"><span>                                    |</span></span>
<span class="line"><span>                                    v</span></span>
<span class="line"><span>                          [PHPStan level 0/1 + baseline]</span></span>
<span class="line"><span>                                    |</span></span>
<span class="line"><span>                                    v</span></span>
<span class="line"><span>                          [PHPUnit / existing test suite]</span></span>
<span class="line"><span>                                    |</span></span>
<span class="line"><span>                                    v</span></span>
<span class="line"><span>                          [PR per module / feature]</span></span>
<span class="line"><span>                                    |</span></span>
<span class="line"><span>                                    v</span></span>
<span class="line"><span>                          [Human review] --&gt; [Merge to main]</span></span></code></pre></div><p><strong>Tool stack:</strong> Rector (OSS), PHPStan (OSS), Composer; optional Cursor/Copilot for manual refactor suggestions on remaining issues.</p><p><strong>Rough cost:</strong></p><ul><li>Infrastructure: none (runs in CI or local).</li><li>API: Rector/PHPStan free; developer time dominant.</li></ul><p><strong>Governance:</strong> Tier 1 or 2 (no PII in codebase). If AI used for suggestions, same “no PII in prompts” rule.</p><p><strong>Failure modes:</strong></p><ul><li>Large unreviewable PRs → scope to file or feature.</li><li>Baseline never reduced → schedule baseline reduction sprints.</li><li>Regressions → gate on test suite and manual smoke test for critical paths.</li></ul><hr><h3 id="poc-3-test-generation-scaffolding-regression" tabindex="-1">PoC 3: Test generation (scaffolding + regression) <a class="header-anchor" href="#poc-3-test-generation-scaffolding-regression" aria-label="Permalink to &quot;PoC 3: Test generation (scaffolding + regression)&quot;">​</a></h3><p><strong>Example architecture (text):</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[Bug fix / feature] --&gt; [Developer requests &quot;test for this&quot;]</span></span>
<span class="line"><span>                                |</span></span>
<span class="line"><span>                                v</span></span>
<span class="line"><span>                    [IDE AI or Cursor/Copilot]</span></span>
<span class="line"><span>                    (scaffold: class, method, basic assertions)</span></span>
<span class="line"><span>                                |</span></span>
<span class="line"><span>                                v</span></span>
<span class="line"><span>                    [Developer adds/edits assertions and edge cases]</span></span>
<span class="line"><span>                                |</span></span>
<span class="line"><span>                                v</span></span>
<span class="line"><span>                    [PHPUnit/Codeception run in CI]</span></span>
<span class="line"><span>                                |</span></span>
<span class="line"><span>                                v</span></span>
<span class="line"><span>                    [PR] --&gt; [Review] --&gt; [Merge]</span></span></code></pre></div><p><strong>Tool stack:</strong> PHPUnit, Codeception (if WP); Cursor, GitHub Copilot, or PhpStorm AI for scaffolds.</p><p><strong>Rough cost:</strong></p><ul><li>Infrastructure: existing CI.</li><li>API: IDE AI subscription (existing); no extra infra.</li></ul><p><strong>Governance:</strong> Tier 2 (client code). No PII in test data or fixtures sent to AI.</p><p><strong>Failure modes:</strong></p><ul><li>Tests that don’t fail when they should → human must add meaningful assertions.</li><li>Coverage gaming → focus on “tests per bug” and failure catch rate, not coverage % alone.</li></ul><hr><h3 id="poc-4-observability-optional-ai-summarisation" tabindex="-1">PoC 4: Observability + optional AI summarisation <a class="header-anchor" href="#poc-4-observability-optional-ai-summarisation" aria-label="Permalink to &quot;PoC 4: Observability + optional AI summarisation&quot;">​</a></h3><p><strong>Example architecture (text):</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[Laravel / WP app] --&gt; [Structured logs + correlation_id]</span></span>
<span class="line"><span>                                |</span></span>
<span class="line"><span>                                v</span></span>
<span class="line"><span>                    [Log aggregation (e.g. Datadog, ELK, CloudWatch)]</span></span>
<span class="line"><span>                                |</span></span>
<span class="line"><span>                    +-----------+-----------+</span></span>
<span class="line"><span>                    v                       v</span></span>
<span class="line"><span>            [Alerts / dashboards]    [Optional: AI summarisation]</span></span>
<span class="line"><span>            (human response)        (redacted log chunk --&gt; LLM --&gt; runbook suggestion)</span></span>
<span class="line"><span>                    |                       |</span></span>
<span class="line"><span>                    v                       v</span></span>
<span class="line"><span>            [Incident response]      [Human uses suggestion or ignores]</span></span></code></pre></div><p><strong>Tool stack:</strong> Laravel logging; WP logging (plugin or custom); Datadog/ELK/CloudWatch; optional gateway + redaction (e.g. PolyRedact, Private AI) before LLM.</p><p><strong>Rough cost:</strong></p><ul><li>Infrastructure: log aggregation ~$50–200/month depending on volume.</li><li>API: LLM for summarisation ~$20–100/month if used; redaction tool extra if used.</li></ul><p><strong>Governance:</strong> Redact PII before any log chunk sent to SaaS LLM; Tier 2 or 3 depending on log content. Document in register.</p><p><strong>Failure modes:</strong></p><ul><li>PII in logs → redaction mandatory before AI; classify log sources.</li><li>Alert fatigue → tune thresholds; prioritise by impact.</li></ul><hr><h3 id="poc-5-client-facing-ai-e-g-internal-search-or-suggestions" tabindex="-1">PoC 5: Client-facing AI (e.g. internal search or suggestions) <a class="header-anchor" href="#poc-5-client-facing-ai-e-g-internal-search-or-suggestions" aria-label="Permalink to &quot;PoC 5: Client-facing AI (e.g. internal search or suggestions)&quot;">​</a></h3><p><strong>Example architecture (text):</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[Client site / app] --&gt; [User query]</span></span>
<span class="line"><span>                                |</span></span>
<span class="line"><span>                                v</span></span>
<span class="line"><span>                    [Backend: auth + rate limit]</span></span>
<span class="line"><span>                                |</span></span>
<span class="line"><span>                                v</span></span>
<span class="line"><span>                    [Query + allowed context] --&gt; [Redaction if needed]</span></span>
<span class="line"><span>                                |</span></span>
<span class="line"><span>                                v</span></span>
<span class="line"><span>                    [On-prem LLM or approved SaaS with DPA]</span></span>
<span class="line"><span>                                |</span></span>
<span class="line"><span>                                v</span></span>
<span class="line"><span>                    [Response] --&gt; [Human-in-loop or auto]</span></span>
<span class="line"><span>                                |</span></span>
<span class="line"><span>                                v</span></span>
<span class="line"><span>                    [Audit log] --&gt; [Client]</span></span></code></pre></div><p><strong>Tool stack:</strong> Self-hosted (Ollama, vLLM, etc.) or approved SaaS (Azure OpenAI, AWS Bedrock in VPC); redaction layer if SaaS; audit logging.</p><p><strong>Rough cost:</strong></p><ul><li>Infrastructure: on-prem GPU or VPC API; variable (£100–500+/month or usage-based).</li><li>API: SaaS usage-based; redaction and audit add cost.</li></ul><p><strong>Governance:</strong> Tier 3. Explicit client approval; legal basis; DPIA if high risk; no raw PII unless contracted and redaction where required.</p><p><strong>Failure modes:</strong></p><ul><li>Scope creep → fix use case and success criteria in contract.</li><li>Hallucinations affecting users → human-in-loop or clear “AI-generated” labelling; fallback to non-AI path.</li></ul><hr><h2 id="step-5-governance-ai-policy-framework" tabindex="-1">STEP 5: Governance &amp; AI Policy Framework <a class="header-anchor" href="#step-5-governance-ai-policy-framework" aria-label="Permalink to &quot;STEP 5: Governance &amp; AI Policy Framework&quot;">​</a></h2><h3 id="_5-1-internal-ai-usage-policy-summary" tabindex="-1">5.1 Internal AI usage policy (summary) <a class="header-anchor" href="#_5-1-internal-ai-usage-policy-summary" aria-label="Permalink to &quot;5.1 Internal AI usage policy (summary)&quot;">​</a></h3><ul><li><strong>Approved tools only</strong> — Use only tools on the approved list for client work.</li><li><strong>Data minimisation</strong> — Send to AI only what is strictly necessary; no client PII in prompts unless Tier 3 approved and redaction where required.</li><li><strong>No training on our/client data</strong> — Contractual no-training commitment from vendors; assume breach for detection.</li><li><strong>Tier 1/2/3</strong> — Tier 1: no client data. Tier 2: client code/config, redacted/synthetic only; client notification + opt-out. Tier 3: client PII or confidential data or client-facing AI; explicit client approval and legal basis.</li><li><strong>Transparency</strong> — Document where and how AI is used; support client approval and audit.</li><li><strong>Secure coding</strong> — AI-assisted SAST/SCA encouraged with approved tools; human review for critical findings.</li></ul><p>Full detail: <a href="./reports/ai-governance/01-governance-framework-draft.html">Governance Framework</a>.</p><hr><h3 id="_5-2-client-approval-model" tabindex="-1">5.2 Client approval model <a class="header-anchor" href="#_5-2-client-approval-model" aria-label="Permalink to &quot;5.2 Client approval model&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Tier</th><th>Criteria</th><th>Approval</th></tr></thead><tbody><tr><td>1</td><td>No client PII; generic tooling on non-client code</td><td>Internal only</td></tr><tr><td>2</td><td>Client code/config; no PII; or redacted/synthetic only</td><td>Client notification + opt-out</td></tr><tr><td>3</td><td>Client PII or confidential data; or client-facing AI</td><td>Explicit client approval (written); documented legal basis</td></tr></tbody></table><p><strong>Workflow:</strong> Request → Risk classification (1/2/3) → Security/compliance review (Tier 2–3) → Client approval (Tier 3) → Register entry → Periodic re-approval (e.g. annual).</p><hr><h3 id="_5-3-data-handling-controls" tabindex="-1">5.3 Data handling controls <a class="header-anchor" href="#_5-3-data-handling-controls" aria-label="Permalink to &quot;5.3 Data handling controls&quot;">​</a></h3><ul><li><strong>Classification:</strong> Identify datasets and flows that contain PII (code comments, configs, fixtures, logs).</li><li><strong>Redaction before SaaS:</strong> Default no raw PII to SaaS AI; use redaction pipeline (e.g. PolyRedact, Private AI, or Presidio) before external calls.</li><li><strong>Single control point:</strong> Prefer gateway or proxy so all traffic to third-party AI passes through one policy and audit.</li><li><strong>Retention:</strong> Align prompt/log retention with vendor commitments and retention policy; secure disposal.</li></ul><hr><h3 id="_5-4-redaction-workflows" tabindex="-1">5.4 Redaction workflows <a class="header-anchor" href="#_5-4-redaction-workflows" aria-label="Permalink to &quot;5.4 Redaction workflows&quot;">​</a></h3><ol><li><strong>Classify</strong> — Which repos, tickets, logs contain PII?</li><li><strong>Detect</strong> — Use consistent PII detection (e.g. 50+ types, multi-language if needed).</li><li><strong>Redact or synthesise</strong> — Mask/hash or synthetic replacement; prefer redaction for external AI.</li><li><strong>Audit</strong> — Log what was redacted and where.</li><li><strong>Centralise</strong> — One control point (proxy/gateway or pre-processing) for all outbound AI traffic.</li></ol><hr><h3 id="_5-5-safe-ai-deployment-checklist" tabindex="-1">5.5 Safe AI deployment checklist <a class="header-anchor" href="#_5-5-safe-ai-deployment-checklist" aria-label="Permalink to &quot;5.5 Safe AI deployment checklist&quot;">​</a></h3><p>Before deploying or expanding any AI use:</p><ul><li>[ ] Tool on approved list and DPA / no-training in place.</li><li>[ ] Tier assigned (1/2/3); Tier 3 has written client approval and legal basis.</li><li>[ ] No raw PII in prompts or payloads unless Tier 3 and redaction policy followed.</li><li>[ ] Human in the loop for critical security decisions and client-facing outputs.</li><li>[ ] Audit trail and register updated.</li><li>[ ] Runbook for “what if AI is wrong or data leaks” (containment, notification, remediation).</li></ul><hr><h2 id="step-6-capability-maturity-model" tabindex="-1">STEP 6: Capability Maturity Model <a class="header-anchor" href="#step-6-capability-maturity-model" aria-label="Permalink to &quot;STEP 6: Capability Maturity Model&quot;">​</a></h2><p>Four levels: from ad hoc AI usage to agentic/semi-autonomous systems. What must be true at each level.</p><table tabindex="0"><thead><tr><th>Level</th><th>Name</th><th>What must be true</th></tr></thead><tbody><tr><td><strong>1</strong></td><td><strong>Ad hoc AI usage</strong></td><td>Some developers use AI (e.g. IDE completion, ad hoc prompts); no standard policy; no central approval or register; risk of shadow AI and inconsistent data handling.</td></tr><tr><td><strong>2</strong></td><td><strong>Assisted development</strong></td><td>Internal AI policy and Tier 1/2/3 in place; approved tool list; PR review or test scaffolding using AI in at least one repo; “no PII in prompts” enforced; client approval for Tier 3.</td></tr><tr><td><strong>3</strong></td><td><strong>AI-augmented workflows</strong></td><td>Structural use across pipeline: AI PR review, test scaffolding, Rector/PHPStan migration, docs from code; observability with optional AI summarisation (redacted); client-facing AI only under Tier 3 and scoped pilots; governance register and re-approval cadence live.</td></tr><tr><td><strong>4</strong></td><td><strong>Agentic / semi-autonomous</strong></td><td>Bounded agentic use (e.g. dependency bump PRs, release packaging) with human gate; clear ownership and rollback; no unconstrained agent access to production or client data; maturity reviewed and roadmap updated.</td></tr></tbody></table><hr><h3 id="progression-criteria-concise" tabindex="-1">Progression criteria (concise) <a class="header-anchor" href="#progression-criteria-concise" aria-label="Permalink to &quot;Progression criteria (concise)&quot;">​</a></h3><ul><li><strong>1 → 2:</strong> Publish policy, approved list, one pilot (PR review or tests), Tier 3 process.</li><li><strong>2 → 3:</strong> Roll out PR review and test workflow; migration pipeline on at least one codebase; observability baseline; documentation pass; client-facing pilot if applicable.</li><li><strong>3 → 4:</strong> At least one bounded agentic pilot with success criteria and incident process; no expansion of agent scope without explicit approval.</li></ul><hr><h2 id="references-and-related-documents" tabindex="-1">References and related documents <a class="header-anchor" href="#references-and-related-documents" aria-label="Permalink to &quot;References and related documents&quot;">​</a></h2><table tabindex="0"><thead><tr><th>Document</th><th>Purpose</th></tr></thead><tbody><tr><td><a href="./reports/ai-governance/01-governance-framework-draft.html">AI Governance Framework</a></td><td>Principles, DLP, redaction, deployment model, client approval, GDPR, roles.</td></tr><tr><td><a href="./reports/ai-governance/02-risk-register-template.html">AI Risk Register Template</a></td><td>Risk register with example risks, controls, assessment lenses.</td></tr><tr><td><a href="./reports/ai-governance/03-tooling-stack.html">Recommended Tooling Stack</a></td><td>DLP, redaction, on-prem/SaaS, approval workflows, secure-coding tools.</td></tr><tr><td><a href="./reports/ai-wp-laravel-pipeline/01-research-document.html">AI in the WordPress + Laravel Pipeline</a></td><td>PR review, refactoring, migration, testing, documentation, 90-day pilot, tool comparison.</td></tr></tbody></table><p><strong>Tools cited in this whitepaper:</strong> Rector, PHPStan, Psalm, Semgrep, SonarQube, CodeRabbit, Graphite Agent, Greptile, PHPUnit, Codeception, Laravel Horizon/Telescope, PolyRedact, Private AI, Presidio, Ollama, vLLM, Dependabot, Renovate.</p><p><em>This programme is for internal planning and client discussion. Adapt to your jurisdiction and contracts; seek legal and compliance sign-off before adoption.</em></p>`,122)])])}const u=e(o,[["render",r]]);export{h as __pageData,u as default};
